[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Clinta Puthussery Varghese",
    "section": "",
    "text": "Welcome! Hello and welcome to my website.\nMy name is Clinta Puthussery Varghese. I am currently pursuing my second semester as a graduate student in Statistics at Zicklin School of Business in the Baruch College, City University Of New York with an anticipated graduation in May 2026.\nMy aspiration is to become a Data Scientist. As beauty lies in the eyes of the beholder, so too does the power of data rest in the hands of the analyst. I am dedicated to equipping myself to work with messy data, uncover insights, and ultimately contribute to making the world a better place. Poke around my website to learn more."
  },
  {
    "objectID": "mini.html",
    "href": "mini.html",
    "title": "Projects",
    "section": "",
    "text": "I investigate the fiscal characteristics of US public transit authorities and explored on What do you believe to be the most efficient transit system in the country?. In this project, I handle the data import and tidying; students are mainly responsible for “single table” dplyr operations (mutate, group_by, summarize, select, arrange, rename) to produce summary statistics.\n\n\n\n\n\n\n\nThis project positioned me as a Hollywood development executive, analyzing large-scale IMDb data ~20 GB to pitch a new movie. I developed proficiency in handling large data sets and relational data structures through techniques like table joins, enhancing my skills in managing and interpreting extensive information. I have also explored gganimate,wordcloud,ggplot libraries in this project.\n\n\n\n\n\n\nIn this analysis, I investigated the impact of different state-level electoral vote allocation methods on presidential election outcomes. I compared various allocation strategies, assessed fairness and proportionality, and created visualizations like red/blue electoral maps using data from the MIT Election Lab and the US Census Bureau’s TIGER repository.\n\n\n\n\n\n\n\nFor this project, I conducted a comparative analysis of two CUNY retirement plans using Monte Carlo simulations. By incorporating historical financial and macroeconomic data, I assessed the probability of different retirement outcomes. This experience included working with APIs, implementing complex simulations, and applying bootstrap inference techniques to quantify prediction uncertainties."
  },
  {
    "objectID": "mini.html#my-academic-projects-in-r",
    "href": "mini.html#my-academic-projects-in-r",
    "title": "Projects",
    "section": "",
    "text": "I investigate the fiscal characteristics of US public transit authorities and explored on What do you believe to be the most efficient transit system in the country?. In this project, I handle the data import and tidying; students are mainly responsible for “single table” dplyr operations (mutate, group_by, summarize, select, arrange, rename) to produce summary statistics.\n\n\n\n\n\n\n\nThis project positioned me as a Hollywood development executive, analyzing large-scale IMDb data ~20 GB to pitch a new movie. I developed proficiency in handling large data sets and relational data structures through techniques like table joins, enhancing my skills in managing and interpreting extensive information. I have also explored gganimate,wordcloud,ggplot libraries in this project.\n\n\n\n\n\n\nIn this analysis, I investigated the impact of different state-level electoral vote allocation methods on presidential election outcomes. I compared various allocation strategies, assessed fairness and proportionality, and created visualizations like red/blue electoral maps using data from the MIT Election Lab and the US Census Bureau’s TIGER repository.\n\n\n\n\n\n\n\nFor this project, I conducted a comparative analysis of two CUNY retirement plans using Monte Carlo simulations. By incorporating historical financial and macroeconomic data, I assessed the probability of different retirement outcomes. This experience included working with APIs, implementing complex simulations, and applying bootstrap inference techniques to quantify prediction uncertainties."
  },
  {
    "objectID": "mp01.html#i.-introduction",
    "href": "mp01.html#i.-introduction",
    "title": "Analyzing Transit Data",
    "section": "I. Introduction",
    "text": "I. Introduction\nThis project is inspired from popular CityNerd Youtube channel’s presentation on Farebox Recovery. The main goal of this mini project is to explore, analyze, and interpret transit data from various sources to derive insights into ridership trends, agency performance, and the financial efficiency of transit systems for the year 2022.\nThe primary source of data is from National Transit Database\nThe datasets used in this analysis is from\n\nThe 2022 Annual Database Fare Revenues table\nThe latest Monthly Ridership tables\nThe 2022 Operating Expenses reports\n\nThe analysis primarily focuses on key financial and operational performance metrics, such as Vehicle Revenue Miles (VRM), Unlinked Passenger Trips (UPT), and Farebox Recovery Ratio (the ratio of total fares to expenses). Additional explorations focus on the most efficient transit modes and the busiest metropolitan areas."
  },
  {
    "objectID": "mp01.html#ii.-data-preparation",
    "href": "mp01.html#ii.-data-preparation",
    "title": "Analyzing Transit Data",
    "section": "II. Data Preparation",
    "text": "II. Data Preparation\n\nLoading Required Libraries\nTo begin, we load the necessary R libraries, primarily using the tidyverse package for data wrangling and DT for data visualization.\n\n\nCode\n#|warning: false\n#|message: false\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"DT\")) install.packages(\"DT\")\nlibrary(tidyverse)\nlibrary(DT)\n\n\n\n\nImporting Datasets\nWe imported three main datasets: Fare Revenue (FARES), Operating Expenses (EXPENSES), and Ridership Data (TRIPS, MILES). These were cleaned and filtered to remove irrelevant columns, resulting in a data set that focuses on total fares, expenses, and VRM/UPT data.\n\n\nCode\n#|warning: false\n#|message: false\n#|results: \"hide\"\nFARES &lt;- readxl::read_xlsx(\"MP-DATA/2022 Fare Revenue.xlsx\")\nEXPENSES &lt;- readr::read_csv(\"MP-DATA/2022_NTD_Annual_Data_-_Operating_Expenses__by_Function__20231102.csv\")\nTRIPS &lt;- readxl::read_xlsx(\"MP-DATA/July 2024 Complete Monthly Ridership (with adjustments and estimates)_240903.xlsx\", sheet=\"UPT\")\nMILES &lt;- readxl::read_xlsx(\"MP-DATA/July 2024 Complete Monthly Ridership (with adjustments and estimates)_240903.xlsx\", sheet=\"VRM\")\n\n\n\n\nData Cleaning\nAfter importing the data, several unnecessary columns were dropped to keep the datasets focused on the required metrics (UPT, VRM, Expenses, etc.)\nTo extract monthly financials such as Total Fares and Expenses for each mode, A new dataset (FINANCIALS) is created by joining Fare Revenue (FARES), Operating Expenses (EXPENSES). It contains (NTD ID, Agency Name , Mode, Total Fares, Expenses)\n\n\nCode\n#|warning: false\n#|message: false\n#|results: hide\n\nFARES &lt;- FARES|&gt;\n    select(-`State/Parent NTD ID`, \n           -`Reporter Type`,\n           -`Reporting Module`,\n           -`TOS`,\n           -`Passenger Paid Fares`,\n           -`Organization Paid Fares`) |&gt;\n    filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n    select(-`Expense Type`) |&gt;\n    group_by(`NTD ID`,       # Sum over different `TOS` for the same `Mode`\n             `Agency Name`,  # These are direct operated and sub-contracted \n             `Mode`) |&gt;      # of the same transit modality\n                             # Not a big effect in most munis (significant DO\n                             # tends to get rid of sub-contractors), but we'll sum\n                             # to unify different passenger experiences\n    summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n    ungroup()\n\nEXPENSES&lt;- EXPENSES|&gt;\n    select(`NTD ID`, \n           `Agency`,\n           `Total`, \n           `Mode`) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n    rename(Expenses = Total) |&gt;\n    group_by(`NTD ID`, `Mode`) |&gt;\n    summarize(Expenses = sum(Expenses)) |&gt;\n    ungroup()\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\n\nSimilarly to extract monthly transits numbers such as Unlinked Passenger Trip and Vehicle Revenue Miles for each agency and mode, a new data set USAGE(USAGE) is created by joining TRIPS (TRIPS) and MILES (MILES). It contains (NTD ID,Agency ,UZA Name,Mode,3 Mode,month,UPT, VRM )\n\n\nCode\n#|warning: false\n#|message: false\n#|results: hide\n#|\nTRIPS &lt;- TRIPS |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"UPT\") |&gt;\n            drop_na() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\nMILES &lt;- MILES |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"VRM\") |&gt;\n            drop_na() |&gt;\n            group_by(`NTD ID`, `Agency`, `UZA Name`, \n                     `Mode`, `3 Mode`, month) |&gt;\n            summarize(VRM = sum(VRM)) |&gt;\n            ungroup() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`))"
  },
  {
    "objectID": "mp01.html#iii.-exploratory-data-analysis-eda",
    "href": "mp01.html#iii.-exploratory-data-analysis-eda",
    "title": "Analyzing Transit Data",
    "section": "III. Exploratory Data Analysis (EDA)",
    "text": "III. Exploratory Data Analysis (EDA)\n\nSampling and Displaying Data\n\nUSAGE\nThe str(str) function is used to get an overview of the datatypes.\n\n\nCode\nstr(USAGE)\n\n\ntibble [281,010 × 8] (S3: tbl_df/tbl/data.frame)\n $ NTD ID  : int [1:281010] 1 1 1 1 1 1 1 1 1 1 ...\n $ Agency  : chr [1:281010] \"King County\" \"King County\" \"King County\" \"King County\" ...\n $ UZA Name: chr [1:281010] \"Seattle--Tacoma, WA\" \"Seattle--Tacoma, WA\" \"Seattle--Tacoma, WA\" \"Seattle--Tacoma, WA\" ...\n $ Mode    : chr [1:281010] \"DR\" \"DR\" \"DR\" \"DR\" ...\n $ 3 Mode  : chr [1:281010] \"Bus\" \"Bus\" \"Bus\" \"Bus\" ...\n $ month   : Date[1:281010], format: \"2002-01-01\" \"2002-02-01\" ...\n $ UPT     : num [1:281010] 135144 127378 136030 142204 144697 ...\n $ VRM     : num [1:281010] 746158 656324 726578 736975 746158 ...\n\n\nFor easier readability, the column names are renamed, for ex: UZA Name can be renamed as metro_area. I have decided to keep UPT and VRM as is since its harder to rename with its full abbreviation.\n\nCreating Syntatic Names\nRename a column: UZA Name to metro_area.\n\n\nCode\n##### Task 1\nUSAGE &lt;- rename(USAGE, \n               metro_area = `UZA Name`\n               )\n\n\n\n\nRecoding the Mode column\nThe details for what each Mode represents were found in National Transit Database (NTD) Glossary\n\n\nCode\n##### TASK 2\nUSAGE &lt;- USAGE |&gt;\n    mutate(Mode = case_when(\n        Mode == \"AR\" ~ \"Alaska Rail\",\n        Mode == \"CB\" ~ \"Commuter Bus\",\n        Mode == \"CC\" ~ \"Cable Car\",\n        Mode == \"CR\" ~ \"Commuter Rail\",\n        Mode == \"DR\" ~ \"Demand Response\",\n        Mode == \"FB\" ~ \"Ferryboat\",\n        Mode == \"HR\" ~ \"Heavy Rail\",\n        Mode == \"IP\" ~ \"Inclined Plane\",\n        Mode == \"LR\" ~ \"Light Rail\",\n        Mode == \"MB\" ~ \"Motor Bus\",\n        Mode == \"MG\" ~ \"Monorail/Automated Guideway\",\n        Mode == \"PB\" ~ \"Publico\",\n        Mode == \"RB\" ~ \"Bus Rapid Transit\",\n        Mode == \"SR\" ~ \"Streetcar Rail \",\n        Mode == \"TB\" ~ \"Trolleybus \",\n        Mode == \"TR\" ~ \"Aerial Tramway\",\n        Mode == \"VP\" ~ \"Vanpool\",\n        Mode == \"YR\" ~ \"Hybrid Rail\",\n        TRUE ~ \"Unknown\"\n    ))\n\n\nTo get an overview of the ridership data (USAGE), we sampled 1000 records and displayed them using an interactive datatable.\n\n\n\n\n\n\n\n\n\nFINANCIALS\n\n\nCode\nstr(FINANCIALS)\n\n\ntibble [1,173 × 5] (S3: tbl_df/tbl/data.frame)\n $ NTD ID     : num [1:1173] 1 1 1 1 1 1 1 1 2 2 ...\n $ Agency Name: chr [1:1173] \"King County Department of Metro Transit\" \"King County Department of Metro Transit\" \"King County Department of Metro Transit\" \"King County Department of Metro Transit\" ...\n $ Mode       : chr [1:1173] \"CB\" \"DR\" \"FB\" \"LR\" ...\n $ Total Fares: num [1:1173] 5216912 832327 1715265 29386480 56846337 ...\n $ Expenses   : num [1:1173] 0.00 6.05e+07 8.90e+06 0.00 6.72e+08 ...\n\n\nFor easier readability the Mode is recoded\nHere is the overview of the ridership data (FINANCIALS),\n\n\nCode\nFINANCIALS |&gt;  DT::datatable()\n\n\n\n\n\n\n\nAnswering Instructor Specified Questions with dplyr.\nA.Vehicle Revenue Miles (VRM) Analysis\n\nVRM refers to Vehicle Revenue Miles.\nIt is the miles that vehicles are scheduled to or actually travel while in revenue service. (total number of miles traveled by a vehicle while it is in service and generating revenue by transporting passengers. It is used in public transportation and transit systems to measure the productive service a vehicle provides.)\nVehicle revenue miles include: Layover / recovery time\nVehicle revenue miles exclude: Deadhead, Operator training,Vehicle maintenance testing, and Other non-revenue uses of vehicles.\n1.Which transit agency had the most total VRM?\n\n\n\nCode\n##### Task 4\n#|warning: false\n#|message: false\n#|results: hide\n#|\ntable_creation&lt;-function(x){\n  datatable(x, \n            options = list(\n    searching = FALSE,   # Removes the search bar\n    pageLength = 10,      # Optional: Set the number of rows displayed per page\n    lengthChange = FALSE,# Removes the option to change the number of rows displayed\n     dom = 't'\n  ),\n  filter = 'none'\n  )  \n}\n\n\nBy summarizing VRM across transit agencies from 2002 to 2024, (I created a function named table_creation to tables)\n\n\nCode\ntable_creation(\n  USAGE %&gt;%\n    group_by(Agency) %&gt;%\n    summarise(`total_VRM($/miles)` = sum(VRM)) %&gt;%\n    arrange(desc(`total_VRM($/miles)`)) %&gt;%\n    slice(1:3))\n\n\n\n\n\n\nThe transit agency with the highest VRM in the sample is identified as\nAgency: MTA New York City Transit with Total_VRM: 10.83 billion revenue miles\n2.Which transit mode had the most total VRM?\nSimilarly, the analysis was performed by transit mode and arranged in descending order.\n\n\nCode\ntable_creation(USAGE %&gt;%\n  group_by(Mode) %&gt;%\n  summarise(`total_VRM($/miles)` = sum(VRM)) %&gt;%\n  arrange(desc(`total_VRM($/miles)`) )|&gt; slice(1:3))\n\n\n\n\n\n\nThe transist Mode (Motor Bus) had the total VRM of (49.45 billion revenue miles)\n3.NYC Subway Ridership in May 2024\nTo analyze ridership on the NYC Subway, we filtered the data for Heavy Rail in May 2024 and retrieved UPT values:\n\n\nCode\ntable_creation(USAGE %&gt;%\n  filter(Mode == 'Heavy Rail' & str_detect(Agency, \"New York City\") & month == \"2024-05-01\") %&gt;%\n  select(Mode,UPT))\n\n\n\n\n\n\nThe monthly ridership of subway for month of May 2025 is total UPT of (180.46 million)\n\n4.NYC Subway Ridership Decline from April 2019 to April 2020\nA significant decline in ridership was observed between April 2019 and April 2020. The percentage decline was calculated, and the ridership trend was plotted:\n\n\nCode\n#|warning: false\n#|message: false\n#|results: hide\n\nNYC_subway &lt;- USAGE %&gt;%\n  filter(month&gt;=\"2019-04-01\", month &lt;= \"2024-04-01\", , Mode == 'Heavy Rail' & str_detect(Agency, \"New York City\"))\n\n# Calculate the percentage decline from April 2019 to April 2020\nstart_value &lt;- NYC_subway %&gt;% filter(month == \"2019-04-01\") %&gt;% pull(UPT)\nend_value &lt;- NYC_subway %&gt;% filter(month == \"2020-04-01\") %&gt;% pull(UPT)\ndecline_percentage &lt;- ((start_value - end_value) / start_value) * 100\n\n# Create the plot\nggplot(NYC_subway, aes(month, UPT / 1000000)) +\n  geom_line(color=\"blue\") +\n  scale_y_continuous(labels = scales::comma) +\n  labs(title = paste(\"NYC Subway Ridership Data (April 2019 - April 2024)\\nDecline: \",\n                     round(decline_percentage, 2), \"% from (04/2019-04/2020)\"),  # Display decline percentage in title\n       x = \"timeline\",\n       y = \"UPT (Unlinked Passenger Trips) in Millions\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe subway system experienced a 91.28% decline in ridership between April 2019 and April 2020.\nHowever, post-pandemic, ridership has been steadily increasing.However,with the rise of hybrid work culture, monthly UPT rides now show a fluctuating, zigzag pattern.\nQuestions that I explore by myself\n\n1. Which are the top 5 metropolitan areas with the highest number of transit agencies?\n\n\nCode\ntable_creation(USAGE |&gt;group_by(metro_area)|&gt;\nsummarize(`number of agencies` =length(unique(Agency)))|&gt;\narrange(desc(`number of agencies`)) |&gt;\nslice(1:5))\n\n\n\n\n\n\nThere are 38 agencies operating in the NY–Jersey City–Newark, NY–NJ metro area and 22 agencies in the Los Angeles–Long Beach, CA region. Let’s focus on the NY–NJ area.\n\n\nCode\n#|warning: false\n#|message: false\n#|results: hide\nUSAGE&lt;-USAGE|&gt;mutate(year=year(month))\nNY_NJ&lt;-USAGE |&gt; \n  filter(metro_area == \"New York--Jersey City--Newark, NY--NJ\") |&gt;\n  group_by(year, Agency,Mode) |&gt; \n  summarize(\n    total_VRM = sum(VRM, na.rm = TRUE), \n    total_UPT = sum(UPT, na.rm = TRUE), \n    \n    ratio = if_else(total_VRM&gt;0, total_VRM,NA_real_) / if_else(total_UPT &gt; 0, total_UPT, NA_real_), .groups = 'drop'\n  )|&gt;ungroup()\n\n\n2 What type of Transist Modes are Offered by Agencies in NY-NJ Metro Area\n\n\nCode\n#|warning: false\n#|message: false\n#|results: hide\n\n# Filter for the year 2023, group by Agency and Mode, and count occurrences\ntable_creation(NY_NJ %&gt;%\n  filter(year == 2023) %&gt;%\n  group_by(Mode) %&gt;%\n  summarize(count = n() )%&gt;%  # Count occurrences of each mode\n  ungroup()|&gt;arrange(desc(count)))\n\n\n\n\n\n\nMost agencies in NY-NJ Metro area offers Bus services. Lets see how is the presence of each mode with in Agencies.\n\n\nCode\n#|warning: false\n#|message: false\n#|results: hide\n\nNY_NJ &lt;- NY_NJ |&gt; \n  mutate(Agency = case_when(\n    Agency== \"Metro-North Commuter Railroad Company, dba: MTA Metro-North Railroad\" ~ \"MTA Metro_North\",\n    Agency== \"New York City Department of Transportation\"~\"NYC_DOT\",\n    Agency== \"New York City Economic Development Corporation\"~\"NYC_EDC\",\n    Agency== \"Staten Island Rapid Transit Operating Authority\"~\"Staten Island Transit\",\n    \n    TRUE ~ Agency  # Keep the original name if no match is found\n  ))\n\n\n\n\nCode\n#|warning: false\n#|message: false\n#|results: hide\nagency_modes_2023 &lt;- NY_NJ %&gt;%\n  filter(year == 2023) %&gt;%\n  group_by(Agency, Mode) %&gt;%\n  summarize(count = n(),.groups=\"drop\") %&gt;%  # Count occurrences of each mode\n  ungroup()\n\n\n\n\nCode\n#|warning: false\n#|message: false\n#|results: hide\n\nggplot(agency_modes_2023, aes(x = Mode, y = Agency)) +\n  geom_point(aes(color = Mode), size = 2)+  # Create a tile for each combination of mode and agency\n  labs(title = \"Presence of Transit Modes by Agency\\n,\n       New York--Jersey City--Newark\",\n       y = \"Agency\", \n       x = \"Transit Modes (2023)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        panel.grid.major = element_line(size = 0.75, color = \"grey70\"),  # Major grid lines\n    panel.grid.minor = element_line(size = 0.5, color = \"grey85\"),  # Minor grid lines for a sharper effect\n    panel.grid.major.x = element_blank())\n\n\n\n\n\n\n\n\n\nIt’s surprising to see that the Vanpool service is exclusively offered by NJT. Given the vastness of the NY-NJ metropolitan area, it would be beneficial to promote Vanpool services similar to how other major cities like the SF-LA area are doing.\n3 Which Mode of transit achieved the highest Vehicle Revenue Miles (VRM) per trip each year.\nThe VRM per trip ratio provides valuable insights into agency efficiency. A lower ratio indicates higher vehicle utilization, meaning the agency is transporting more passengers per mile of service. Typically, urban areas tend to have lower VRM per trip ratios compared to rural areas.\nTo assess the efficiency of agencies in the NY–NJ metropolitan area, we calculated the ratio of VRM to Unlinked Passenger Trips (UPT). This ratio reflects how well transit systems are utilizing their vehicles in relation to passenger demand, with a lower VRM/UPT ratio indicating better efficiency.\n4.Which Transist Mode is profitable in termns of VRM/UPT Ratio:\n\n\nCode\n#|warning: false\n#|message: false\n#|results: hide\n\nggplot(NY_NJ|&gt;filter(year&gt;=2002,year&lt;=2024)|&gt;group_by(Agency,Mode)|&gt;ungroup(), aes(x = ratio, y = Mode,fill=Mode)) +\n  geom_boxplot() +\n  labs(title = \"Distribution of VRM/UPT Ratios by Transit Mode from 2002 - 2024\",\n       y = \"Transit Mode\", \n       x = \"VRM/UPT Ratio\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n#|warning: false\n#|message: false\n#|results: hide\n\ncat(\"## Median Ratio by Agency and Mode (Up to 2024)\\n\")\n\n\n## Median Ratio by Agency and Mode (Up to 2024)\n\n\nCode\ntable_creation(\n  NY_NJ|&gt;filter(year&lt;=2024)|&gt;\n    group_by(Agency,Mode)|&gt;\n    summarize(median_ratio=round(median(ratio,na.rm=TRUE),2),.groups=\"drop\")|&gt;\n                arrange(median_ratio))\n\n\n\n\n\n\n\n\nCode\n#|warning: false\n#|message: false\n#|results: hide\n\ncat(\"## Table for mean of median VRM/UPT ratios for each mode in NY-NJ Metro Area Till 2024\")\n\n\n## Table for mean of median VRM/UPT ratios for each mode in NY-NJ Metro Area Till 2024\n\n\nCode\ntable_creation(\n# Calculate the mean of median ratios for each mode from 2021 to 2024\nmean_median_ratios &lt;- NY_NJ %&gt;%\n  filter(year &lt;= 2024) %&gt;%\n  group_by(Agency, Mode) %&gt;%\n  summarize(median_ratio = median(ratio, na.rm = TRUE), .groups = 'drop') %&gt;%  # Calculate median ratio for each agency and mode\n  group_by(Mode) %&gt;%\n  summarize(mean_of_median = round(mean(median_ratio, na.rm = TRUE),2),no_agency=n(), .groups = 'drop')|&gt;arrange(mean_of_median))  # Calculate mean of median ratios for each mode)\n\n\n\n\n\n\nFerryboat service and heavy rail service has the lowest VRM/UPT ratio after factoring no-agencies contributing to the data, This means those modes have better operational efficiency in terms of ridership relative to miles driven. Since there is only 1 agency contributing to the bus rapid transist, it is hard to determine its universality. :::"
  },
  {
    "objectID": "mp01.html#iv.-financial-and-usage-data-analysis",
    "href": "mp01.html#iv.-financial-and-usage-data-analysis",
    "title": "Analyzing Transit Data",
    "section": "IV. Financial and Usage Data Analysis",
    "text": "IV. Financial and Usage Data Analysis\n\nCombining USAGE and FINANCIALS\nAnalysing some data about financial recovery\nThe USAGE and FINANCIALS datasets were combined, and as a threshold total UPT &gt; 400000 is been taken into consideration.\n\n\nCode\n#|warning: false\n#|message: false\n#|results: hide\n\nUSAGE_2022_ANNUAL&lt;- USAGE |&gt; filter(year==\"2022\")|&gt;\n  group_by(`NTD ID`,\nAgency,\nmetro_area,\nMode)|&gt;\n  summarize(\n    total_UPT=sum(UPT,na.rm = TRUE),\n    total_VRM=sum(VRM,na.rm = TRUE),.groups=\"drop\"\n  )|&gt;\n  ungroup()\n\nUSAGE_AND_FINANCIALS &lt;- left_join(USAGE_2022_ANNUAL, \n           FINANCIALS, \n           join_by(`NTD ID`, Mode)) |&gt;\n    drop_na()|&gt;select(-`Agency Name`)\nUSAGE_AND_FINANCIALS_top&lt;-USAGE_AND_FINANCIALS |&gt; filter(total_UPT&gt;=400000) \n\n\n\n\nCode\nUSAGE_AND_FINANCIALS_top|&gt;DT:: datatable()\n\n\n\n\n\n\n\n\nFarebox Recovery and Cost Efficiency in Transit Systems\n\n1. Transit System with the Highest Farebox Recovery For The Year 2022\nFarebox recovery is the ratio of Total Fares to Total Expenses, measuring how effectively fare revenue covers a transit system’s operational costs. It’s crucial for a transit agency to generate enough revenue from passengers to offset its operating expenses. A higher farebox recovery ratio reduces the agency’s reliance on alternative funding sources to keep the transit system running.\n\n\nCode\ntable_creation(USAGE_AND_FINANCIALS_top |&gt; \n  group_by(Agency, Mode) |&gt; \n  summarize(\n    fare_box_ratio = round(ifelse(`Total Fares` &gt; 0, `Total Fares`, NA_real_) / ifelse(Expenses &gt; 0, Expenses, NA_real_),2\n  )) |&gt; \n  arrange(desc(fare_box_ratio)) |&gt; \n  ungroup() |&gt; \n  slice(1:5))\n\n\n\n\n\n\nAgency: Port Imperial Ferry Corporation\nMode: Ferry Boat\nFarebox Recovery Ratio: 1.43\nInteresting Facts about Port Imperial Ferry’s Role\nIn 2022, the system with the highest farebox recovery ratio was the Port Imperial Ferry Corporation.\n\n\nPrime Location: Port Imperial terminal, located in Weehawken, New Jersey, connects commuters from New Jersey’s Hudson River waterfront to key locations in Manhattan. It is a crucial transportation link, especially for daily commuters.\n\nIntermodal Hub: It is also part of an intermodal hub, with connections to New Jersey Transit’s Hudson-Bergen Light Rail. This makes it easier for commuters to switch transit modes, enhancing the system’s convenience.\n\nTime-Saving Option: Their primary customers are daily commuters as it is often faster than other transit modes during rush hours, avoiding heavy traffic on bridges and tunnels, making it a preferred option for those seeking efficiency.\n\nSummary: Port Imperial Ferry’s high farebox recovery is driven by its limited competition, strategic docking agreements, loyal commuter base, and efficient cost management, allowing the service to generate significant revenue relative to its operational costs.\n\n\n\n\n2. Transit System with the Lowest Expenses per Unlinked Passenger Trip (UPT)\nExpenses per UPT measure the cost efficiency of a transit system, indicating how much the agency spends to serve per unlinked passenger trip.\n\n\nCode\ntable_creation(USAGE_AND_FINANCIALS_top |&gt; \n  group_by(Agency, Mode) |&gt; \n  summarize(\n    expenses_per_UPT = round(ifelse(Expenses &gt; 0, Expenses, NA_real_) / total_UPT\n  ,2)) |&gt; \n  arrange(expenses_per_UPT) |&gt; \n  ungroup() |&gt; \n  slice(1:5))\n\n\n\n\n\n\nAgency: North Carolina State University\nMode: Motor Bus\nExpenses per UPT: 1.18 $/ride\n\n\nWhy NCSU’s Motorbus System is Profitable?\n\nCampus Size: North Carolina State University’s main campus spans over 2,000 acres. This vast area creates the need for an internal transportation network to efficiently connect different parts of the university.\nTransportation Master Plan: NCSU has a well-structured Transportation Master Plan aimed at improving and optimizing its transportation network. This plan includes strategies to enhance route efficiency, reduce congestion, and ensure the system meets the growing demands of the campus population.\nInstitutional Support and Subsidies: NCSU likely subsidizes a portion of the motor bus system’s operating costs, which helps keep operational expenses lower. University funding or student fees may cover some costs, reducing the financial burden on passengers while keeping fares affordable, if not free.\nHigh Ridership: With a high volume of students and staff commuting daily, the system benefits from economies of scale. High passenger volume distributes operational costs across more riders, making the cost per trip lower, and ensuring the bus system operates efficiently.\nFocused Cost Management: NCSU’s internal control over the transportation system allows for focused cost management. With optimized vehicle maintenance, route planning, and operational schedules, the motor bus system is kept cost-efficient, ensuring its financial sustainability.\nSummary: NCSU’s motor bus system is profitable due to the combination of a large and spread-out campus, optimized and efficient route planning, institutional subsidies, and high ridership levels. This strategic approach, combined with NCSU’s Transportation Master Plan, ensures that the system operates at a low cost, making it not only cost-efficient but also potentially profitable.\n\n\n\n\n3. Transit System with the Highest Total Fares per UPT\nThe highest total fares per UPT indicate the system that generates the most fare revenue per passenger trip.\n\n\nCode\ntable_creation(USAGE_AND_FINANCIALS_top |&gt; \n  group_by(Agency, Mode) |&gt; \n  summarize(\n    totalfares_per_UPT = round(ifelse(`Total Fares` &gt; 0, `Total Fares`, NA_real_) / total_UPT\n  ,2)) |&gt; \n  arrange(desc(totalfares_per_UPT)) |&gt; \n  ungroup() |&gt; \n  slice(1:5))\n\n\n\n\n\n\nAgency: Hampton Jitney, Inc\nMode: Commuter Bus\nFares per UPT: 41.3 $/mile\nHampton Jitney is a commuter bus company.  Their Three primary routes from the east end of Long Island (The Hamptons and the North Fork) to New York City. Hampton Jitney also operates charter and tour services, along with local transit bus service in eastern Suffolk County under contract with Suffolk County Transit.\n\n\nWhy Hampton Jitney, Inc. has the highest fare per UPT?\n\nWealthy Customer Base : The Hampton Jitney serves affluent passengers traveling between New York City and the Hamptons, a popular destination for wealthy individuals. These passengers are generally less sensitive to price and are willing to pay premium fares for a convenient, comfortable ride.\n\nPrivate Agency : As a private transportation service, Hampton Jitney is not bound by government fare controls or subsidies. This allows the agency to charge market-based fares that reflect the demand and exclusive nature of the service.\n\nPremium Service : The Hampton Jitney offers luxury features such as comfortable seating, Wi-Fi, and direct routes, which justify the higher fare prices. Customers are paying not just for transportation, but for an upscale, stress-free experience.\nConvenient and Direct Routes: The bus service offers direct transportation from Manhattan to the Hamptons, saving passengers the hassle of driving or taking multiple transfers on public transportation. This convenience is a major factor in the willingness of passengers to pay higher fares.\nSeasonal Demand: The Hamptons is a popular summer destination, and during peak seasons, demand for transportation to and from the area skyrockets. Hampton Jitney can charge premium fares during these high-demand periods, further increasing their fare per UPT.\n\nConclusion: Hampton Jitney’s high fare per ride is driven by its affluent customer base, premium service offerings, convenient routes, and its ability to charge market-driven prices as a private agency. The high fare reflects the value that customers place on convenience and comfort, particularly when traveling to a luxury destination like the Hamptons.\n\n\n\n\n\n4. Transit System with the Lowest Expenses per Vehicle Revenue Mile (VRM)\nThis metric shows the agency that operates most efficiently in terms of expenses for each mile their vehicles are in service.\n\n\nCode\ntable_creation(USAGE_AND_FINANCIALS_top |&gt; \n  group_by(Agency, Mode) |&gt; \n  summarize(\n    expenses_per_VRM = round(ifelse(Expenses &gt; 0, Expenses, NA_real_) / ifelse(total_VRM &gt; 0, total_VRM, NA_real_),2)\n  ) |&gt; \n  arrange(expenses_per_VRM) |&gt; \n  ungroup() |&gt; \n  slice(1:5))\n\n\n\n\n\n\nAgency: Metropolitan Transportation Commission\nMode: Vanpool\nExpenses per VRM: 0.445 $/mile\n**Metro-Area*:** San Francisco--Oakland, CA\nThe Bay Area Vanpool Program, managed by the MTC, supports groups of 7 to 15 commuters traveling together with an unpaid driver.\n\nAccording to the Berkleyside, Casual Carpool was a Bay Area tradition before COVID. Post Covid, longtime riders and drivers who want to revive casual carpool are finding it difficult to reestablish the famously organic tradition. But since many people are returning to work and seeking efficient ways to travel, and it’s interesting to observe that there’s a slow resurgence of this informal carpooling tradition.\n\nWhy MTC’S Vanpool reduce expenses per Vehicle Revenue Mile (VRM)?\n\nCost-Effective Commute : Vanpooling is often more economical than driving alone, with participants sharing the costs of fuel, tolls, and maintenance. This can lead to significant savings for commuters.\n\nSustainable Way : By reducing the number of single-occupancy vehicles on the road, vanpools help decrease traffic congestion and lower greenhouse gas emissions, contributing to a more sustainable environment.\n\nVanpool Rewards : The MTC actively promotes vanpooling as part of its broader strategy to enhance public transportation options and reduce reliance on individual car travel. Each counties provides different benefits including pre-tax benefits, discounted parking permits and subsidies for commuter vanpoolers\n\n\n\n\n\n5. Transit System with the Highest Total Fares per VRM\n\nThe highest total fares per VRM represent the system that generates the most fare revenue for each mile that its vehicles are in service.\n\n\n\nCode\ntable_creation(USAGE_AND_FINANCIALS_top |&gt; \n  group_by(Agency, Mode) |&gt; \n  summarize(\n    fares_per_VRM = round(ifelse(`Total Fares` &gt; 0, `Total Fares`, NA_real_) / ifelse(total_VRM &gt; 0, total_VRM, NA_real_),2)\n  ) |&gt; \n  arrange(desc(fares_per_VRM)) |&gt; \n  ungroup() |&gt; \n  slice(1:5))\n\n\n\n\n\n\nAgency: Jacksonville Transportation Authority\nMode: Ferryboat\nFares per VRM: 157.70 $/mile\nThe St. Johns River Ferry is an important transportation link, providing service across the St. Johns River and facilitating commuter travel."
  },
  {
    "objectID": "mp01.html#v.-what-do-you-believe-to-be-the-most-efficient-transit-system-in-the-country",
    "href": "mp01.html#v.-what-do-you-believe-to-be-the-most-efficient-transit-system-in-the-country",
    "title": "Analyzing Transit Data",
    "section": "V. What do you believe to be the most efficient transit system in the country?",
    "text": "V. What do you believe to be the most efficient transit system in the country?\nIn my view, the most efficient transit system is one that prioritizes the needs of the community rather than focusing solely on generating revenue. Such a system aims to provide reliable and accessible transportation options that serve the public effectively. Among the various transit modes analyzed, I find ferryboats to be particularly efficient, especially in terms of their overall Vehicle Revenue Miles (VRM) to Unlinked Passenger Trips (UPT) ratio. This efficiency indicates that ferryboats are capable of serving a substantial number of passengers relative to the distance traveled, making them a viable option for enhancing urban mobility.\nAdditionally, when considering cost-effectiveness and environmental sustainability, the Vanpool mode emerges as the best option. Vanpools not only reduce operational costs but also contribute positively to the environment by minimizing the number of individual vehicles on the road. By consolidating passengers into fewer vehicles, Vanpools can significantly decrease carbon emissions and traffic congestion, promoting a greener transit solution.\nUltimately, an efficient transit system should not merely aim for financial gain but should instead focus on fulfilling the transportation needs of its users while fostering sustainable practices. By investing in transit options like ferryboats and Vanpools, cities can create a more effective and environmentally friendly transportation network that benefits both the community and the planet."
  },
  {
    "objectID": "mp01.html#vi.-appendix",
    "href": "mp01.html#vi.-appendix",
    "title": "Analyzing Transit Data",
    "section": "VI. Appendix",
    "text": "VI. Appendix\nAdditional data and visualizations can be provided upon request, including full code listings and intermediate data tables."
  },
  {
    "objectID": "MP-DATA/dataverse_files2/codebook-us-president-1976-2020.html",
    "href": "MP-DATA/dataverse_files2/codebook-us-president-1976-2020.html",
    "title": "CLINTA PUTHUSSERY VARGHESE",
    "section": "",
    "text": "#Codebook for U.S. President Returns 1976–2016\nThe data file 1976-2016-president contains constituency (state-level) returns for elections to the U.S. presidency from 1976 to 2016. The data source is the document “Statistics of the Congressional Election,” published biennially by the Clerk of the U.S. House of Representatives.\n##Variables The variables are listed as they appear in the data file.\n###year - Description: year in which election was held\n\n###office - Description: U.S. PRESIDENT\n\n###state - Description: state name\n\n###state_po - Description: U.S. postal code state abbreviation\n\n###state_fips - Description: State FIPS code\n\n###state_cen - Description: U.S. Census state code\n\n\nstate_ic\n\nDescription: ICPSR state code\n\n\n\n\ncandidate\n\nDescription: name of the candidate\nNote: The name is as it appears in the House Clerk report.\n\n\n\n\nparty_detailed\n\nDescription: party of the candidate (always entirely uppercase)\n\nNote: Parties are as they appear in the House Clerk report. In states that allow candidates to appear on multiple party lines, separate vote totals are indicated for each party. Therefore, for analysis that involves candidate totals, it will be necessary to aggregate across all party lines within a district. For analysis that focuses on two-party vote totals, it will be necessary to account for major party candidates who receive votes under multiple party labels. Minnesota party labels are given as they appear on the Minnesota ballots. Future versions of this file will include codes for candidates who are endorsed by major parties, regardless of the party label under which they receive votes.\n\n\n\n\nparty_simplified\n\nDescription: party of the candidate (always entirely uppercase) The entries will be one of: DEMOCRAT, REPUBLICAN, LIBERTARIAN, OTHER\n\n\n\n\nwritein\n\nDescription: vote totals associated with write-in candidates\nCoding:\n\n|:–|:–| | “TRUE” | write-in candidates | | “FALSE” | non-write-in candidates |\n\n\n\ncandidatevotes\n\nDescription: votes received by this candidate for this particular party\nNote: Massachusetts and New York returns often contains entries for “blank,” “other/blank,” “scattering/blank,” and the like. For analyses that depend on an accurate count of votes cast for candidates (rather than turnout, including blank ballots), consult state returns. Future versions of this dataset will distinguish blank ballots from votes cast for scattering candidates.\n\n\n\n\ntotalvotes\n\nDescription: total number of votes cast for this election\n\n\n\n\nversion\n\nDescription: The date the dataset was last updated, in the format yyyymmdd\n\n\n\n\nmode\n\nDescription: The way in which the vote was cast: for example, whether the vote was cast absentee, on election day, or early."
  },
  {
    "objectID": "MP-DATA/dataverse_files/codebook-us-house-1976–2020.html",
    "href": "MP-DATA/dataverse_files/codebook-us-house-1976–2020.html",
    "title": "CLINTA PUTHUSSERY VARGHESE",
    "section": "",
    "text": "#Codebook for U.S. House Returns 1976–2020\nThe data file 1976-2020-house contains constituency (district) returns for elections to the U.S. House of Representatives from 1976 to 2020. The data source is the document “Statistics of the Congressional Election,” published biennially by the Clerk of the U.S. House of Representatives. 2018 data comes from official state election websites, and for Kansas, come from Stephen Pettigrew and the Kansas Secretary of State office (in some cases, they are marked as unofficial, and will be updated at a later time).\nAll string variables are in upper case.\n##Variables The variables are listed as they appear in the data file.\n###year - Description: year in which election was held\n\n###state - Description: state name\n\n###state_po - Description: U.S. postal code state abbreviation\n\n###state_fips - Description: State FIPS code\n\n###state_cen - Description: U.S. Census state code\n\n\nstate_ic\n\nDescription: ICPSR state code\n\n\n###office - Description: U.S. House (constant)\n\n\n\ndistrict\n\nDescription: district number\n****Note****: At-large districts are coded as 0 (zero).\n\n\n\n\nstage\n\nDescription: electoral stage\nCoding:\n\n\n\n\ncode\ndefinition\n\n\n\n\n“gen”\ngeneral elections\n\n\n“pri”\nprimary elections\n\n\n\n\nNote: Only appears in special cases. Consult original House Clerk report for these cases.\n\n\n\n\nspecial\n\nDescription: special election\nCoding\n\n\n\n\ncode\ndefinition\n\n\n\n\n“TRUE”\nspecial elections\n\n\n“FALSE”\nregular elections\n\n\n\n\n\n\ncandidate\n\nDescription: name of the candidate\nNote: The name is as it appears in the House Clerk report.\n\n\n\n\nparty\n\nDescription: party of the candidate (always entirely lowercase)\n\nNote: Parties are as they appear in the House Clerk report. In states that allow candidates to appear on multiple party lines, separate vote totals are indicated for each party. Therefore, for analysis that involves candidate totals, it will be necessary to aggregate across all party lines within a district. For analysis that focuses on two-party vote totals, it will be necessary to account for major party candidates who receive votes under multiple party labels. Minnesota party labels are given as they appear on the Minnesota ballots. Future versions of this file will include codes for candidates who are endorsed by major parties, regardless of the party label under which they receive votes.\n\n\n\n\n\nwritein\n\nDescription: vote totals associated with write-in candidates\nCoding:\n\n\n\n\ncode\ndefinition\n\n\n\n\n“TRUE”\nwrite-in candidates\n\n\n“FALSE”\nnon-write-in candidates\n\n\n\n\n\n\nmode\n\nDescription: mode of voting; states with data that doesn’t break down returns by mode are marked as “total”\n\n\n\n\ncandidatevotes\n\nDescription: votes received by this candidate for this particular party\n\n\n\n\ntotalvotes\n\nDescription: total number of votes cast for this election\n\n\n\n\nfusion_ticket\n\nDescription: A TRUE/FALSE indicator as to whether the given candidate is running on a fusion party ticket, which will in turn mean that a candidate will appear multiple times, but by different parties, for a given election. States with fusion tickets include Connecticut, New Jersey, New York, and South Carolina.\n\n\n\n\n\n\n\n## unofficial Description: TRUE/FALSE indicator for unofficial result (to be updated later); this appears only for 2018 data in some cases\n\n\n\n\n\nversion\n\nDescription: date when this dataset was finalized\n\n##NOTES:\ncandidatevotes: for uncontested races, value is set to 1 in FL. Should user want to set a higher value for analysis purposes, consider setting the value as the maximum for a given state-year. The code in R would be the following: df &lt;- read.csv(“1976-2018-house.csv”, stringsAsFactors = FALSE) df &lt;- df %&gt;% group_by(state_po,district) %&gt;% mutate(max_st_year_vote = max(candidatevotes, na.rm=T)\nThe following code should be used if the user would like to assume that uncontested candidates would have recieved as many votes as the best contested candidate.\ndistrict: district is set to 0 for single member states.\nparty and candidate: candidate - party combinations are recorded as they were on the state elections website. This means that for states where the same candidate might appear on multiple parties, like in NY, they are recorded as such. Therefore, for users interested in finding the primary party, run the following code:\ndf &lt;- read.csv(“1976-2020-house.csv”, stringsAsFactors = FALSE) df\\(district &lt;- str_pad(df\\)district, width=2, pad=“0”, side=“left) df\\(state_fips &lt;- str_pad(df\\)state_fips, width=2, pad=”0”, side=“left) df\\(GEOID &lt;- paste(df\\)state_fips, df$district, sep=”“) df_max &lt;- df %&gt;% group_by(candidate, GEOID, year) %&gt;% slice(which.max(candidatevotes) df_sum &lt;- df %&gt;% group_by(candidate, GEOID, year) %&gt;% aggregate(candvotes_sum = sum(candvotes))"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini Project #2: IMDB Dataset Analysis",
    "section": "",
    "text": "I.Introduction\nI believe everyone enjoys a good movie, though what defines a “good film” can vary from person to person—a debate in itself. In this project, I aim to conduct an exploratory data analysis using the the IMDb non-commercial release to uncover insights into movie popularity, ratings, and trends in genre popularity across decades. Ultimately, my goal is to propose a movie remake based on these findings.\nAs part of this project, I’ll take on the role of a Hollywood development executive. Traditionally, these executives would acquire life rights for “based on a true story” films, secure options for promising novels, or adapt existing intellectual property (IP). However, in recent years, this process has been criticized for relying too heavily on sequels and reboots. To break away from that pattern, I will use data-driven insights to identify what makes a film successful, analyze the key players in film making, and examine notable Hollywood flops. By understanding these patterns, I’ll aim to propose fresh, innovative movie ideas that resonate with today’s audience.\n\n\nII.Data Preparation\nThe following packages are used for this analysis: dplyr, tidyr, DT, ggplot2 and tidyverse,gganimate , scales. If these packages have not been installed in the system, they can be with the following code:\n\n1.Loading Required Libraries\nTo begin, we load the necessary R libraries, primarily using the tidyverse package for data wrangling and DT for data visualization.\n\n\nCode\n#|message: false \n#|warning: false\n#|code-fold: true\nif(!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"DT\")) install.packages(\"DT\")\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(ggplot2)\n\n\n\n\nCode\n#|message: false \n#|warning: false\n#|code-fold: true\n#I have also created a function for interactive table creation\ntable_creation&lt;-function(x){\n  datatable(x, \n            options = list(\n              searching = FALSE,   # Removes the search bar\n              pageLength = 10,      # Optional: Set the number of rows displayed per page\n              lengthChange = FALSE,# Removes the option to change the number of rows displayed\n              dom = 't'\n            ),\n            filter = 'none'\n  )  \n}\n\n\n\n\n2.Data Loading\nUsing the get_imdb_filefunction we are loading all the required DATASETS\n\n\nCode\n#|eval: false\n\nget_imdb_file &lt;- function(fname){\n    BASE_URL &lt;- \"https://datasets.imdbws.com/\"\n    fname_ext &lt;- paste0(fname, \".tsv.gz\")\n    if(!file.exists(fname_ext)){\n        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n        download.file(FILE_URL, \n                      destfile = fname_ext)\n    }\n    as.data.frame(readr::read_tsv(fname_ext, lazy=FALSE))\n}\n\n\n\n\nCode\n#|eval: false\nTITLE_BASICS&lt;- get_imdb_file(\"title.basics\")\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title.principals\")\nTITLE_EPISODES   &lt;- get_imdb_file(\"title.episode\")\nNAME_BASICS &lt;- get_imdb_file(\"name.basics\")\nTITLE_RATINGS    &lt;- get_imdb_file(\"title.ratings\")\nTITLE_CREW       &lt;- get_imdb_file(\"title.crew\")\n\n\n\n\nCode\nNAME_BASICS&lt;-read.csv( \"MP-DATA/NAME_BASICS.csv\")\n\nTITLE_BASICS&lt;-read.csv( \"MP-DATA/TITLE_BASICS.csv\")\n\nTITLE_EPISODES&lt;-read.csv( \"MP-DATA/TITLE_EPISODES.csv\")\nTITLE_RATINGS&lt;-read.csv( \"MP-DATA/TITLE_RATINGS.csv\")\nTITLE_CREW&lt;-read.csv( \"MP-DATA/TITLE_CREW.csv\")\nTITLE_PRINCIPALS1&lt;-read.csv( \"MP-DATA/TITLE_PRINCIPALS.csv\")\n\n\n\n\n3.Data Sub-Sampling\nThe NAME_BASICS dataset is filtered to only include rows where the knownForTitles column contains more than one title (indicated by the presence of more than one comma). This step ensures that only individuals known for multiple works are retained.\nSimilarly, TITLE_RATINGS are filtered with numVotes&gt;100.\n\n\nCode\n#|eval: false\n#|code-fold: true\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n    filter(str_count(knownForTitles, \",\") &gt; 1)\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n    filter(numVotes &gt;= 100)\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(parentTconst == tconst))\n\nTITLE_EPISODES &lt;- bind_rows(TITLE_EPISODES_1,\n                            TITLE_EPISODES_2) |&gt;\n    distinct()\n\nTITLE_PRINCIPALS1 &lt;- TITLE_PRINCIPALS1 |&gt; semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\n\n\n\n\nIII.Initial Exploration\n\nCorrect the column types of the TITLE BASICS tables using a combination of mutate and the coercion functions as.numeric and as.logical..\n\n\nCode\n#### Task 1: Column Type Correction\n#| message: false \n#| warning: false\n#| code-fold: true\n#| eval: false\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n    mutate(startYear = as.numeric(startYear))\nTITLE_BASICS&lt;-TITLE_BASICS |&gt; separate_longer_delim(genres, \",\") \n\n\nSimilarly, the Genres column needs to be cleaned. I decided to keep only relevant genres and rest are classified as others. genre_cleaned includes: \"Action\", \"Comedy\", \"Drama\", \"Horror\", \"Romance\", \"Thriller\", \"Adventure\", \"Animation\", \"Biography\", \"Crime\", \"Documentary\",\"Musical\",\"Romance\",\"Sci-Fi\"\n\n\nCode\nrelevant_genres &lt;- c(\"Action\", \"Comedy\", \"Drama\", \"Horror\", \"Romance\", \"Thriller\", \"Adventure\", \"Animation\", \"Biography\", \"Crime\", \"Documentary\",\"Musical\",\"Romance\",\"Sci-Fi\")\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  mutate(genre_cleaned = case_when(\n    genres %in% relevant_genres ~ genres,\n    TRUE ~ \"Others\"  # Assign \"Others\" to all non-relevant genres\n  ))\n\n\nNAME BASICS also needs the birthYear and deathYear changed to as.numeric\n\n\nCode\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n    mutate(birthYear = as.numeric(birthYear),\n           deathYear = as.numeric(deathYear))\n\n\nTITLE_CREW has multiple directors names in a single cell. it has to be separated.\n\n\nCode\nTITLE_CREW&lt;-TITLE_CREW |&gt; separate_longer_delim(directors, \",\")\n\n\nTITLE EPISODES also needs the season number and episode number changed to numeric.\n\n\nCode\nTITLE_EPISODES&lt;-TITLE_EPISODES|&gt;mutate(\n            seasonNumber=as.numeric(seasonNumber),\n           episodeNumber=as.numeric(episodeNumber))\n\n\n\n\nCode\n# Install the necessary packages\nif (!requireNamespace(\"DiagrammeR\", quietly = TRUE)) {\n  install.packages(\"DiagrammeR\")\n}\n\nlibrary(DiagrammeR)\n\n# Create the ER diagram using Graphviz DOT notation\nerd &lt;- grViz(\"\ndigraph ERD {\n  graph [rankdir=LR]\n\n  # Entity Definitions\n  TITLE_BASICS [label = 'TITLE_BASICS\\\\n(tconst, titleType, genres, startYear)']\n  TITLE_RATINGS [label = 'TITLE_RATINGS\\\\n(tconst, averageRating, numVotes)']\n  TITLE_PRINCIPALS [label = 'TITLE_PRINCIPALS\\\\n(tconst, nconst, category)']\n  TITLE_EPISODES [label = 'TITLE_EPISODES\\\\n(tconst, parentTconst, seasonNumber, episodeNumber)']\n  NAME_BASICS [label = 'NAME_BASICS\\\\n(nconst, primaryName, birthYear, deathYear, knownForTitles)']\n  TITLE_CREW [label = 'TITLE_CREW\\\\n(tconst, directors, writers)']\n\n  # Relationships\n  TITLE_BASICS -&gt; TITLE_RATINGS [label = '1:1']\n  TITLE_BASICS -&gt; TITLE_PRINCIPALS [label = '1:N']\n  TITLE_BASICS -&gt; TITLE_EPISODES [label = '1:N']\n  TITLE_BASICS -&gt; TITLE_CREW [label = '1:1']\n  NAME_BASICS -&gt; TITLE_PRINCIPALS [label = '1:N']\n  TITLE_EPISODES -&gt; TITLE_BASICS [label = 'N:1 (parent)']\n}\n\")\n\n# Render the diagram\nerd\n\n\n #### Task 2: Instructor-Provided Questions\n\nHow many movies are in our data set? How many TV series? How many TV episodes?\nWho is the oldest living person in our data set?\nThere is one TV Episode in this data set with a perfect 10/10 rating and at least 200,000 IMDb ratings. What is it? What series does it belong to?\nWhat four projects is the actor Mark Hamill most known for?\nWhat TV series, with more than 12 episodes, has the highest average rating?\nThe TV series Happy Days (1974-1984) gives us the common idiom “jump the shark”. The phrase comes from a controversial fifth season episode (aired in 1977) in which a lead character literally jumped over a shark on water skis. Idiomatically, it is used to refer to the moment when a once-great show becomes ridiculous and rapidly looses quality. Is it true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\n\n1.Number of Projects in the TITLE_BASICS\n\n\nCode\ntable(TITLE_BASICS$titleType)\n\n\n\n       movie        short    tvEpisode tvMiniSeries      tvMovie     tvSeries \n      266029        39986       394516        12413        29156        60368 \n     tvShort    tvSpecial        video    videoGame \n        1052         4388        17491        11972 \n\n\nThere are mutliple entries for movies(TV movie, movies) and series (TV Series, series). Hence, I wrote a functioncount_title to identify the project type and count.\n\n\nCode\n#|message: false \n#|warning: false\n#|code-fold: true\ncount_title&lt;-function(x){\n  word&lt;-x\n  count&lt;-sum(grepl(word,TITLE_BASICS$titleType,ignore.case = TRUE))\n  return(count)                        \n}\n\n\nI used the count_title function to identify movies,series,episodes in the TITLE_BASICS dataset\n\n\nCode\n#|message: false \n#|warning: false\n#|code-fold: true\nmovie_count&lt;-count_title(\"movie\")\nseries_count&lt;-count_title(\"series\")\nepisode_count&lt;-count_title(\"episode\")\nprint(paste0(\"There are \", movie_count, \" movies, \", series_count, \" series and \", episode_count, \" episodes in the TITLE_BASICS dataset.\"))\n\n\n[1] \"There are 295185 movies, 72781 series and 394516 episodes in the TITLE_BASICS dataset.\"\n\n\n2.Oldest Person Alive According to wikipedia, the oldest person alive in the world is of 1908 onwards. Hence, I keep checked for people who are born on 1908 and afterwards, and who doesn’t a death year\n\n\nCode\n#|message: false \n#|warning: false\n#|code-fold: true\nNAME_BASICS|&gt;select(-knownForTitles)|&gt;\n  filter(birthYear==1908, is.na(deathYear))|&gt;\n  distinct(primaryName, .keep_all = TRUE)|&gt;select(primaryName,primaryProfession,birthYear)|&gt;DT::datatable()\n\n\n\n\n\n\nThere are 111 individuals who were born after 1908 and still alive for birth year .\n3.PERFECT RATED EPISODE\n\n\nCode\nlibrary(DT)\n\nperfect_episode&lt;-TITLE_RATINGS|&gt;filter(averageRating==10.0,numVotes&gt;=200000)\n\nperfect_episode&lt;-perfect_episode|&gt; left_join(TITLE_EPISODES,by=c('tconst'='tconst'))\n\nperfect_episode&lt;-perfect_episode|&gt;left_join(TITLE_BASICS,by=c('tconst'='tconst'))\n\ntable_creation(perfect_episode|&gt;\n                 select(originalTitle,titleType,seasonNumber,episodeNumber,averageRating,numVotes)|&gt;distinct())\n\n\n\n\n\n\nThe perfect rated TV episode is Ozymandias which is the 14th Episode in Season 5 of Breaking Bad TV Series.\n4.FIND PROJECTS OF ANY ACTOR OR DIRECTOR\nI wrote a function find_projects which takes actor_or_director name as a parameter and returns all the projects they have been part of.\nI began by joining the TITLE_RATINGS and TITLE_EPISODES datasets to create TITLE_BASICS_RATING_1 and TITLE_BASICS_RATING_2, using the tconst (title constants) as the key. This ensured that all relevant title ratings and episode information were combined.\nNext, I merged these two datasets into TITLE_RATING_EPISODE, arranged it by descending averageRating and numVotes, and removed duplicate entries to get a clean dataset for further analysis.\n\n\nCode\nlibrary(tidyverse)\nTITLE_BASICS_RATING_1&lt;-full_join(TITLE_RATINGS,TITLE_EPISODES|&gt;select(tconst),by=c(\"tconst\" = \"tconst\"))\nTITLE_BASICS_RATING_2&lt;-full_join(TITLE_RATINGS,TITLE_EPISODES|&gt;select(parentTconst)|&gt;rename(tconst=parentTconst),by=c(\"tconst\" = \"tconst\"))\nTITLE_RATING_EPISODE&lt;-bind_rows(TITLE_BASICS_RATING_1,TITLE_BASICS_RATING_2)\nTITLE_RATING_EPISODE&lt;-TITLE_RATING_EPISODE|&gt;\n  arrange(desc(averageRating),desc(numVotes))|&gt;distinct()\nrm(TITLE_BASICS_RATING_1)\nrm(TITLE_BASICS_RATING_2)\n\n\nI created the dataset ALL_TITLES by merging title information (such as primaryTitle, titleType, and startYear) with the TITLE_RATING_EPISODE. I filtered out any entries categorized under the genre “Others” to focus on specific genres.\n\n\nCode\n#|code-summary: \"Show the code\"\n\nALL_TITLES&lt;-full_join(\n  TITLE_BASICS|&gt;\n    select(tconst,primaryTitle,titleType,genres,genre_cleaned,startYear),\n  TITLE_RATING_EPISODE,by=c(\"tconst\" = \"tconst\"))|&gt;select(-genres)|&gt;filter(genre_cleaned!=\"Others\")\nsample_n(ALL_TITLES,100)|&gt;DT::datatable()\n\n\n\n\n\n\nI gathered crew information by combining TITLE_CREW (containing director data) and TITLE_PRINCIPALS1 (with principal actor information) to create ALL_CREW, which included both directors and actors.\n\n\nCode\n#|code-summary: \"Show the code\"\n\nTITLE_RATINGS_CREW_1&lt;-ALL_TITLES|&gt;select(tconst)|&gt;full_join(TITLE_CREW|&gt;select(tconst,directors),by=c(\"tconst\"=\"tconst\"))|&gt;distinct()\n\nTITLE_RATING_CREW_2&lt;-ALL_TITLES|&gt;select(tconst)|&gt;full_join(TITLE_PRINCIPALS1|&gt;select(tconst,nconst),by=c(\"tconst\"=\"tconst\"))|&gt;distinct()\n\nALL_CREW=bind_rows(TITLE_RATINGS_CREW_1|&gt;rename(nconst=directors),TITLE_RATING_CREW_2)\nrm(TITLE_RATINGS_CREW_1,TITLE_RATING_CREW_2)\n\nsample_n(ALL_CREW,10)|&gt; DT::datatable()\n\n\n\n\n\n\nTo link this data with individual crew members, I merged the ALL_CREW dataset with the NAME_BASICS dataset, which includes actors and directors known projects and names.\n\n\nCode\n#|code-summary: \"Show the code\"\nlibrary(tidyverse)\nALL_CREW=ALL_CREW|&gt;full_join(NAME_BASICS|&gt;select(nconst,primaryName,knownForTitles),by=c(\"nconst\"=\"nconst\"))\nALL_CREW&lt;-ALL_CREW|&gt;distinct()\nsample_n(ALL_CREW,10)|&gt;DT::datatable()\n\n\n\n\n\n\nHere is the Find_Projects function:\n\n\nCode\n#|code-summary: \"Show the code\"\nfind_projects&lt;-function(actor_or_director){\n  titles_1&lt;-ALL_CREW |&gt; \n    filter(str_detect(primaryName, actor_or_director))|&gt;\n    select(nconst,knownForTitles,tconst)\n  titles_1&lt;-titles_1|&gt;separate_longer_delim(knownForTitles, \",\") \n  titles&lt;-vctrs::vec_c(titles_1$tconst|&gt;unique(),titles_1$knownForTitles|&gt;unique())|&gt;unique()\n\n found_projects&lt;-ALL_TITLES|&gt;filter(tconst %in% titles)\n   \n \n  \n return(found_projects)\n}\n\n\n\n\nCode\n# Create a flowchart for the workflow\ngrViz(\"\ndigraph workflow {\n  graph [rankdir=TB]\n\n  # Nodes\n  TITLE_RATINGS [label = 'TITLE_RATINGS', shape = box]\n  TITLE_EPISODES [label = 'TITLE_EPISODES', shape = box]\n  TITLE_BASICS [label = 'TITLE_BASICS', shape = box]\n  ALL_TITLES [label = 'ALL_TITLES\\\\nMerged Dataset', shape = ellipse]\n  TITLE_CREW [label = 'TITLE_CREW', shape = box]\n  TITLE_PRINCIPALS1 [label = 'TITLE_PRINCIPALS1', shape = box]\n  NAME_BASICS [label = 'NAME_BASICS', shape = box]\n  ALL_CREW [label = 'ALL_CREW\\\\nCrew Data', shape = ellipse]\n  Find_Projects [label = 'find_projects()\\\\nFunction', shape = diamond]\n\n  # Edges\n  TITLE_RATINGS -&gt; ALL_TITLES\n  TITLE_EPISODES -&gt; ALL_TITLES\n  TITLE_BASICS -&gt; ALL_TITLES\n  TITLE_CREW -&gt; ALL_CREW\n  TITLE_PRINCIPALS1 -&gt; ALL_CREW\n  NAME_BASICS -&gt; ALL_CREW\n  ALL_TITLES -&gt; Find_Projects\n  ALL_CREW -&gt; Find_Projects\n}\n\")\n\n\n 4.1: MARK HAMIL’S FAMOUS PROJECTS\nI developed a function called find_projects that takes the name of an actor or director as input (in this case, “Mark Hamill”). The function filters the combined dataset (ALL_CREW) to find all projects associated with the specified person, including those in the knownForTitles column. The function then searches for these titles in the ALL_TITLES dataset to return the relevant projects\n\n\nCode\nname=\"Mark Hamill\"\nfind_projects(name)|&gt;select(primaryTitle,averageRating,numVotes)|&gt;distinct()|&gt;\n  arrange(desc(numVotes),desc(averageRating))|&gt;slice(1:75)|&gt;DT::datatable()\n\n\n\n\n\n\nBased on the above analysis and functions, we find that Mark Hamill is famously known for the following four film/TV Series projects, which are Star Wars movies, The Batman Animated movies,Scooby Doo Animated Series and Avatar:The Last Airbender animated series. Mark Hamill is prestigious to be included in these film projects as they are highly rated and loved by the fans, especially Star Wars movies, The Last Airbender Animated Series and the Batman Animated movies.\n5.HIGHEST RATED SERIES\nI joined ALL_TITLES and TITLE_EPISODES to create a new dataset known as ’SERIES`\n\n\nCode\nSERIES &lt;- right_join(\n  ALL_TITLES,\n  TITLE_EPISODES |&gt;\n    na.omit() |&gt; \n    filter(episodeNumber &gt; 12)|&gt;\n    distinct(parentTconst, .keep_all = TRUE) |&gt; # Keep distinct rows based on  \n    select(-tconst),                # Drop the tconst column\n  by = c(\"tconst\" = \"parentTconst\") )\n\n\nI used word cloud to print the titles based on the number of Votes\n\n\nCode\nlibrary(wordcloud)\n\n\nSERIES&lt;-SERIES|&gt;arrange(desc(numVotes),desc(averageRating))|&gt;select(primaryTitle,averageRating,numVotes)|&gt;distinct()|&gt;slice(1:25)\n\nwordcloud(words = SERIES$primaryTitle, freq = SERIES$numVotes, \n          random.order = FALSE, colors = brewer.pal(8, \"Dark2\"),\n           scale = c(4, 0.5), border = \"black\", \n          fixed.asp = TRUE, use.r.layout = TRUE)\n#Add a border by plotting a rectangle\nrect(-1, -1, 1, 1, border = \"black\", lwd = 5)\n\n\n\n\n\n\n\n\n\n\n\nCode\nSERIES|&gt;DT::datatable(caption = \"Highly Rated Series\")\n\n\n\n\n\n\nAs seen from the above table, Breaking Bad ranks the first among the top ten TV series, that has the highest number of votes and rated well by the critics. It deserves to be rated as the highest because of the thrilling intensity, it gives the viewers as well as the way they show each of the character development and growth through each of the episode and season.\n\n\nIV.Quantifying Success\n\nTask 3: Custom Success Metric\nDesign a ‘success’ measure for IMDb entries, reflecting both quality and broad popular awareness. Implement your success metric using a mutate operator to add a new column to the TITLE_RATINGS table.\nValidate your success metric as follows:\n\nChoose the top 5-10 movies on your metric and confirm that they were indeed box office successes.\nChoose 3-5 movies with large numbers of IMDb votes that score poorly on your success metric and confirm that they are indeed of low quality.\nChoose a prestige actor or director and confirm that they have many projects with high scores on your success metric.\nPerform at least one other form of ‘spot check’ validation.\nCome up with a numerical threshold for a project to be a ‘success’; that is, determine a value \\(v\\) such that movies above \\(v\\) are all “solid” or better.\n\n1.SUCCESS METRICS\nDesign a ‘success’ measure for IMDb entries, reflecting both quality and broad popular awareness. Implement your success metric using a mutate operator to add a new column to the TITLE_RATINGS table.\nI created a new variable, new_rating, using the Smoothing Formula:\nIn simple terms, when I watch a movie, there are two outcomes: either I like it or I don’t. I add my vote as +1 and 2 to the denominator. This method uses a simplified Bayesian average to handle uncertainty in ratings with few votes. The formula is: The formula used is:\n\\[ new rating=(averageRating×numVotes+1)/(numVotes+2) \\]\n\\[ new-rating= (numVotes+2)(averageRating×numVotes+1) \\]\nThe term averageRating * numVotes gives the total sum of the ratings. Adding 1 to the numerator is a form of smoothing to slightly increase ratings with low votes. Dividing by numVotes + 2 accounts for the additional “pseudo-votes” introduced by the smoothing factor.\n\n\nCode\nALL_TITLES&lt;-ALL_TITLES|&gt;mutate(new_rating=round((averageRating*numVotes+1)/(numVotes+2),3))\n\n\n2.High Votes, High Ratings: The Movies Everyone Watched And Everyone Liked\nI used the function identify_title to filter movies\n\n\nCode\nidentify_title&lt;-function(df,word){\n  x&lt;-df|&gt;filter(grepl(word,titleType,ignore.case = TRUE))\n  return(x)                     \n}\n\n\nhighest_top_50 function was developed to identify and retrieve the top 50 movies based on the number of votes and ratings. This function takes a dataframe as input, arranges the data in descending order by numVotes and new_rating, and returns the top 50 entries while excluding the columns averageRating, tconst, and titleType from the output.\n\n\nCode\nhighest_top_50&lt;-function(df)\n{\n  df|&gt;\n    arrange(desc(numVotes),desc(new_rating))|&gt;\n    slice(1:50)|&gt;\n    select(-averageRating,-tconst,-titleType)\n}\n\n\nNext, the dataset movies_rated was generated by using the identify_title function to filter movie titles from the ALL_TITLES dataset, ensuring only distinct rows were included, and removing the genre_cleaned column. The highest_top_50 function was then applied to this dataset, and the result was passed to the table_creation function, which produced an interactive table using the DT package to display the top 50 movies based on votes and ratings.\n\n\nCode\nlibrary(DT)\nmovies_rated&lt;-identify_title(ALL_TITLES,\"movie\")\ntable_creation(highest_top_50(movies_rated|&gt;select(-genre_cleaned,-X)|&gt;distinct()))\n\n\n\n\n\n\n3.High Votes, Low Ratings: The Movies Everyone Watched But No One Liked\nTo identify movies with a large number of IMDb votes but poor performance on the success metric, I sorted the movies_rated dataset by descending order of numVotes and filtered for movies with a new_rating of less than 5. The following code selects the top 10 movies fitting this criterion and excludes the columns tconst and titleType:\n\n\nCode\n# Sort by the new rating\ntable_creation(movies_rated |&gt; \n    arrange(desc(numVotes)) |&gt; \n    filter(new_rating&lt;5) |&gt; \n        select( -tconst, -titleType,-genre_cleaned,-X,-averageRating)|&gt;slice(1:20)|&gt;distinct())\n\n\n\n\n\n\n4.TOM HANKS’s MOVIES\nI have chosen Tom Hanks to check my metrics. Tom Hanks is my favorite Actor. He is a successful actor with numerous oscar wins. List Tom Hanks’ top films and assess their success based on metrics like IMDb ratings, box office gross, and award wins. For example, Forrest Gump, Saving Private Ryan, and Cast Away have not only high IMDb scores but also substantial box office performance and critical acclaim. Lets see if those results are seen\n\n\nCode\nlibrary(dplyr)\n# Function to get user input and find their projects\n#name &lt;- readline(prompt = \"Enter the actor or director name:\")\n\n\n# Assuming `find_projects()` is a function that takes a name and returns a data frame of projects\nname=\"Tom Hanks\"\n\n# Select relevant columns\nfind_projects(name)|&gt;select(primaryTitle,new_rating,numVotes)|&gt;distinct()|&gt;\n  arrange(desc(new_rating),desc(numVotes))|&gt;DT::datatable(caption = \"Tom Hanks Projects\")\n\n\n\n\n\n\nHere is the Visual representation of Tom Hanks successes based on genres. Hanks has won Oscars for Forrest Gump and Philadelphia and has been nominated for several other roles. By evaluating the IMDb rating of his Oscar-nominated films, you can confirm that his roles consistently perform well by your chosen metrics.\n\n\nCode\n#|code-summary: \"Show the code\"\nlibrary(dplyr)\nlibrary(ggplot2)\nfind_projects(name) |&gt;\n  filter(new_rating &gt; median(averageRating)) |&gt;\n  group_by(genre_cleaned)|&gt;\n  summarize(count = n(), .groups = \"drop\")|&gt;\n  ggplot(aes(x = reorder(genre_cleaned, count), y = count, fill = genre_cleaned)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    x = \"Genre\",\n    y = \"Number of Movies per Genre\",\n    fill = \"Genre\"\n  ) +\n  scale_fill_manual(\"Genres\",values = c(\"Documentary\" = \"#1f77b4\", \"Comedy\" = \"#ff7f0e\", \n                               \"Horror\" = \"#2ca02c\", \"Action\" = \"#d62728\", \n                               \"Adventure\" = \"#9467bd\", \"Crime\" = \"#8c564b\", \n                               \"Animation\" = \"#e377c2\", \"Drama\" = \"#7f7f7f\", \n                               \"Romance\" = \"#bcbd22\", \"Sci-Fi\" = \"#17becf\",\n                               \"Thriller\" = \"#1ae4e2\", \"Biography\" = \"#377eb8\",\n                               \"Musical\" = \"#4daf4a\"))+\n  ggtitle(\"Tom Hank's Successful Projects \\n Rating &gt; Median(Rating)\") +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.x = element_blank(),\n    plot.title = element_text(hjust = 0.5)  # centers the title\n  )\n\n\n\n\n\n\n\n\n\n5.Determining the Cutoff: What Makes a Project ‘Solid’?\n\n\nCode\n summary(ALL_TITLES$numVotes)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    100     178     380    4918    1189 2945751 \n\n\nThe data appeared right-skewed, indicating that most movies had relatively low vote counts while a few received a significantly higher number of votes.\nTo better analyze the data, I arranged the ALL_TITLES dataset by both numVotes and new_rating. I then divided the dataset into four quantiles based on the numVotes using the ntile function. This categorization allows for better comparison among movies with varying vote counts.\nI created a density plot using ggplot2 to visualize the distribution of new_rating across the different vote quantiles. This visual representation helps identify how ratings are distributed in relation to the number of votes, offering insight into which movies are considered “solid” based on their vote counts.\n\n\nCode\nggplot(ALL_TITLES, aes(x = new_rating, fill = factor(vote_quantile))) +\n  geom_density(alpha = 0.6) +\n  labs(title = \"Density of Ratings by Vote Quantile\",\n       x = \"Rating\",\n       fill = \"Vote Quantile\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nI defined a success metric in the dataset by creating a new column, success_metrics. Movies are assigned a value of 1 if they fall into thehighest vote quantile (quantile 4) and have a new_rating greater than the median new_rating. Otherwise, they are assigned a value of 0. This approach helps categorize movies as “successful” or “not successful” based on a combination of their rating and the number of votes they received.\n\n\nCode\nALL_TITLES &lt;- ALL_TITLES |&gt;\n  mutate(success_metrics = case_when(\n    new_rating &gt; median(new_rating) & vote_quantile==4 ~ 1,\n    TRUE ~ 0\n  ))\nrm(TITLE_EPISODES_2)\n\n\n\n\nCode\nsample_n(ALL_TITLES,10)|&gt;DT::datatable()\n\n\n\n\n\n\n\n\n\nV.Examining Success by Genre and Decade\n\nTask 4: Trends in Success Over Time\nUsing questions like the following, identify a good “genre” for your next film. You do not need to answer these questions precisely, but these are may help guide your thinking.\n\nWhat was the genre with the most “successes” in each decade?\nWhat genre consistently has the most “successes”? What genre used to reliably produced “successes” and has fallen out of favor?\nWhat genre has produced the most “successes” since 2010? Does it have the highest success rate or does it only have a large number of successes because there are many productions in that genre?\nWhat genre has become more popular in recent years?\n\n1.Charting Success: Which Genre Dominated Each Decade?\nI filtered the ALL_TITLES dataset to create a new dataset called success_projects, containing only the titles marked as successful (where success_metrics equals 1). This data set was then prepared for further analysis. I also ensured the startYear column was numeric for accurate processing and examined a sample of successful projects.\n\n\nCode\nsuccess_projects=ALL_TITLES|&gt;filter(success_metrics==1)|&gt; \n  mutate(startYear = as.numeric(startYear))\nsample_n(success_projects,100)|&gt;DT::datatable()\n\n\n\n\n\n\n\n\nCode\nsummary(success_projects$startYear)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1878    2003    2013    2008    2018    2024 \n\n\nAfter summarizing the startYear, I removed any duplicate entries and filtered out projects with the genre “Others” to focus on distinct genres. A new column, decade, was created based on the startYear. This column categorizes the years into decades, allowing for easier analysis of trends over time.\n\n\nCode\nsuccess_decade&lt;-success_projects|&gt;identify_title(\"movie\")|&gt;select(startYear,primaryTitle,genre_cleaned)|&gt;\n  arrange(startYear)|&gt;\n  distinct()|&gt;\n  filter(genre_cleaned!=\"Others\")\n\nsuccess_decade &lt;- success_decade |&gt; \n  mutate(decade = case_when(\n  startYear &lt; 1940 ~ \"Before 1940\",\n  startYear &gt;= 1940 & startYear &lt; 1950 ~ \"40's\",\n  startYear &gt;= 1950 & startYear &lt; 1960 ~ \"50's\",\n  startYear &gt;= 1960 & startYear &lt; 1970 ~ \"60's\",\n  startYear &gt;= 1970 & startYear &lt; 1980 ~ \"70's\",\n  startYear &gt;= 1980 & startYear &lt; 1990 ~ \"80's\",\n  startYear &gt;= 1990 & startYear &lt; 2000 ~ \"90's\",\n  startYear &gt;= 2000 & startYear &lt; 2010 ~ \"2000's\",\n  startYear &gt;= 2010 & startYear &lt; 2020 ~ \"2010's\",\n  startYear &gt;= 2020 & startYear &lt; 2030 ~ \"2020's\",\n  TRUE ~ \"N/A\"\n))|&gt;mutate(decade = factor(decade, levels = c(\"Before 1940\", \"40's\", \"50's\", \"60's\", \n                                            \"70's\", \"80's\", \"90's\", \"2000's\", \n                                            \"2010's\", \"2020's\", \"N/A\")))\n\n\nThe resulting cleaned dataset, success_decade, now contains distinct entries of successful movies with their corresponding start years and genres categorized by decade.\n\n\nCode\nlibrary(tidyverse)\nsample_n(success_decade,10)|&gt;DT::datatable(caption = \"sample of Success Decade Dataset\")\n\n\n\n\n\n\nTo visual total number of movies based on genre\n\n\nCode\nsuccess_decade |&gt;\n  group_by(decade, genre_cleaned) |&gt;\n  summarize(genre_count = n(), .groups = \"drop\") |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = reorder(genre_cleaned,genre_count), y =genre_count, fill = genre_cleaned)) + \n  geom_bar(stat = \"identity\", position = \"dodge\") +   # Use bars for better visualization\n  facet_wrap(~decade, scales = \"free_y\") + \n  # Facet by decade to see trends   \n  scale_fill_manual(\"Genres\",values = c(\"Documentary\" = \"#1f77b4\", \"Comedy\" = \"#ff7f0e\", \n                               \"Horror\" = \"#2ca02c\", \"Action\" = \"#d62728\", \n                               \"Adventure\" = \"#9467bd\", \"Crime\" = \"#8c564b\", \n                               \"Animation\" = \"#e377c2\", \"Drama\" = \"#7f7f7f\", \n                               \"Romance\" = \"#bcbd22\", \"Sci-Fi\" = \"#17becf\",\n                               \"Thriller\" = \"#1ae4e2\", \"Biography\" = \"#377eb8\",\n                               \"Musical\" = \"#4daf4a\"))+ #              # Use a color-blind-friendly palette\n  labs(title = \"Number of Movies by Genre Across Decades\",\n       y = \"Number of Movies\", \n       x = \"Decade\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1,size = 6))\n\n\n\n\n\n\n\n\n\nHere is a word cloud of the genres based on the counts\n\n\nCode\n#|warning: false\n\nwords&lt;-success_decade |&gt;\n  group_by(decade, genre_cleaned) |&gt;\n  summarize(genre_count = n(), .groups = \"drop\") |&gt;\n  ungroup()\n  \nwordcloud(words = words$genre_cleaned, freq = words$genre_count, \n          random.order = FALSE, colors = brewer.pal(8, \"Dark2\"),\n           scale = c(4, 0.5), \n          fixed.asp = TRUE, use.r.layout = TRUE)\n\n\n\n\n\n\n\n\n\nIt is Drama category is been the most successes in all decades.\n2.The Genre That Never Fails: A Consistent Success Story\nTo analyze the trends in movie genres over time regarding their success, we can examine the average ratings by genre for each decade. This will help us identify which genre has consistently had the most successes and which genre has fallen out of favor over the years.\n\n\nCode\nALL_TITLES &lt;- ALL_TITLES|&gt; mutate(decade = case_when(\n  startYear &lt; 1940 ~ \"Before 1940\",\n  startYear &gt;= 1940 & startYear &lt; 1950 ~ \"40's\",\n  startYear &gt;= 1950 & startYear &lt; 1960 ~ \"50's\",\n  startYear &gt;= 1960 & startYear &lt; 1970 ~ \"60's\",\n  startYear &gt;= 1970 & startYear &lt; 1980 ~ \"70's\",\n  startYear &gt;= 1980 & startYear &lt; 1990 ~ \"80's\",\n  startYear &gt;= 1990 & startYear &lt; 2000 ~ \"90's\",\n  startYear &gt;= 2000 & startYear &lt; 2010 ~ \"2000's\",\n  startYear &gt;= 2010 & startYear &lt; 2020 ~ \"2010's\",\n  startYear &gt;= 2020 & startYear &lt; 2030 ~ \"2020's\",\n  TRUE ~ \"unknown\"\n)) |&gt;\nmutate(decade = factor(decade, levels = c(\"Before 1940\", \"40's\", \"50's\", \"60's\", \n                                            \"70's\", \"80's\", \"90's\", \"2000's\", \n                                            \"2010's\", \"2020's\")))\n\n\n\nALL_MOVIES&lt;-ALL_TITLES|&gt;\n  identify_title(\"movie\")|&gt;\n  select(startYear,decade,primaryTitle,genre_cleaned)|&gt;\n  arrange(startYear)|&gt;\n  distinct()|&gt;\n  filter(genre_cleaned!=\"Others\")|&gt;\n    left_join(ALL_TITLES|&gt;\n                select(startYear,primaryTitle,genre_cleaned,new_rating,success_metrics,numVotes),\n                by=c(\"startYear\"=\"startYear\",\n\"primaryTitle\"=\"primaryTitle\",\"genre_cleaned\"=\"genre_cleaned\"))\n\n\nI then created a new dataset, ALL_MOVIES, which consists of movie titles along with their respective genres, start years, and ratings.\nI calculated the average rating for each genre per decade and filtered out the 2020s to focus on earlier trends. This summary provides insight into how each genre performed over the decades.\n\n\nCode\n# Calculate average rating per genre per decade\nALL_MOVIES |&gt;\n  group_by(genre_cleaned, decade) |&gt;\n  summarise(Average_Rating = median(new_rating))|&gt;ungroup()|&gt;filter(decade!=c(\"2020's\"))|&gt;na.omit()|&gt;\n  \n\nggplot( aes(x = decade, y = Average_Rating, color = decade)) +\n  geom_point() +\n  geom_line(aes(group = genre_cleaned))+\n  facet_wrap(~genre_cleaned, scales = \"free_y\") +  # Facet by genre\n  labs(title = \"Average Movie Rating by Genre and Decade\",\n       x = \"Decade\",\n       y = \"Average Rating\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\n\nDocumentry,Biography and Animation genres consistently produce movies with higher average ratings, typically above 6.5./ On the other hand, Sci-fi,Horror,Thriller present higher risks in terms of audience reception and ratings.These genres were particularly popular in the 1940s and 1950s, a time when they captivated audiences with imaginative storytelling and thrilling experiences. However, over the decades, the landscape of cinema has evolved, and audiences’ tastes have shifted./\nThroughout the decades, Drama has emerged as a dominant genre, enjoying sustained popularity across various eras. However, recent trends indicate that the audience’s appetite for dramatic storytelling may be waning.\n3.Quantity vs. Quality: The Genre with the Most Successes Since 2010\nThe analysis reveals the total number of successes for each genre within the specified time frame. By grouping the data by genre_cleaned and success_metrics, we determined how many projects in each genre were classified as successful (success_metrics = 1) compared to non-successful (success_metrics = 0).\n\n\nCode\n# Load necessary library for text annotations\nlibrary(scales)\n\n# Calculate percentages within each genre for success_metrics\nx&lt;-ALL_MOVIES |&gt;\n  filter(decade %in% c(\"2010's\", \"2020's\")) |&gt;\n  group_by(genre_cleaned, success_metrics) |&gt;\n  summarise(count = n())|&gt;\n  mutate(total = sum(count),  # Total per genre\n         percentage = round((count / total) * 100,2)) |&gt;ungroup()\nx|&gt;DT::datatable()\n\n\n\n\n\n\n\n\nCode\n# Create the plot\nggplot(x, aes(x = reorder(genre_cleaned,total), y=total,fill=as.factor(success_metrics))) +\n  geom_bar(stat = \"identity\", position = \"stack\") +  # Bars are dodged side-by-side for success_metrics\n                # Add percentage labels on top of bars\n  labs(title = \"Total number of sucessful genre for 2010's and 2020's\",\n       x = \"Genre\",\n       y = \"Total Number of Movies\",\n       fill = \"Success\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text( hjust = 1,angle=90))\n\n\n\n\n\n\n\n\n\nAt a glance, the analysis indicates that the Drama genre has produced the most successful projects since 2010, followed closely by Comedy and Thriller. However, when we examine the success rate by considering the ratio of successful projects to the total number of projects within each genre, the results shift significantly.\n\n\nCode\n# Create the plot\nggplot(x|&gt;filter(success_metrics==1), aes(x = reorder(genre_cleaned,percentage), y=percentage,fill=genre_cleaned)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +  # Bars are dodged side-by-side for success_metrics\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\")),\n            position = position_dodge(width = 0.2), vjust = -0.1) +  # Add percentage labels on top of bars\n  labs(title = \"Percentage of sucessful genre for 2010's and 2020's\",\n       x = \"Genre\",\n       y = \"Percentage of successful movies(%)\",\n       fill = \"Genre\") +\n  theme_minimal() +\n  scale_fill_manual(\"Genres\",values = c(\"Romance\" = \"#1f77b4\", \"Comedy\" = \"#ff7f0e\", \n                               \"Horror\" = \"#2ca02c\", \"Action\" = \"#d62728\", \n                               \"Adventure\" = \"#9467bd\", \"Crime\" = \"#8c564b\", \n                               \"Animation\" = \"#e377c2\", \"Drama\" = \"#7f7f7f\", \n                               \"Documentary\" = \"#bcbd22\", \"Sci-Fi\" = \"#17becf\",\n                               \"Thriller\" = \"#1ae4e2\", \"Biography\" = \"#377eb8\",\n                               \"Musical\" = \"#4daf4a\"))+\n  theme(axis.text.x =element_blank())+\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nBiography, Animation, and Documentary genres emerge as the leaders in terms of success metrics when we look at the proportion of successful films. The disparity between the total number of successful projects in Drama, Comedy, and Thriller versus the success rates in Biography, Animation, and Documentary indicates that while traditional genres like Drama may produce a higher volume of successful films, newer or less conventional genres are achieving a greater proportion of success relative to their output. This suggests that these genres are not only producing fewer films but are also prioritizing quality and engagement, leading to better ratings and audience reception.\n4.Emerging Favorites: The Rise of [Genre] in Recent Years\n\n\nCode\nif (!require(\"gganimate\")) install.packages(\"gganimate\")\nlibrary(gganimate)\nif (!require(\"scales\")) install.packages(\"scales\")\nlibrary(scales)\nif (!require(\"zoo\")) install.packages(\"zoo\")\nlibrary(zoo)\n\n\n\n\nCode\nyears &lt;- seq(from = 1950, to = 2020, by = 1)\nyears_df &lt;- data.frame(startYear = years)\nALL_MOVIES&lt;-ALL_MOVIES|&gt;mutate(startYear=as.numeric(startYear))\nALL_MOVIES_1 &lt;- full_join(ALL_MOVIES|&gt;filter(startYear&gt;1950), years_df, by = \"startYear\")|&gt;\n  replace_na(list(column_name = 0))  \n\n\n\n\nCode\nALL_MOVIES_1$startYear &lt;- na.approx(ALL_MOVIES_1$startYear)  \nanim &lt;- ALL_MOVIES_1 |&gt;\n  filter(startYear &gt; 1950) |&gt;\n  ggplot(aes(x= new_rating, y= numVotes, color = genre_cleaned)) +\n  geom_point() +\n  scale_y_log10(labels = scales::comma) +  # Log scale for y-axis\n  scale_x_log10(labels = scales::comma) +  # Log scale for x-axis\n  guides(color = \"none\", size = \"none\") +  # Remove legends for color and size\n  theme_bw() +  # Use a clean white background theme\n  facet_wrap(~genre_cleaned) +  # Facet by genre_cleaned\n  ylab(\"Number of Votes\") +\n  xlab(\"Ratings\") +\n  transition_time(startYear) +  # Animate by rounded years\n  ggtitle(\"NumVotes Vs Ratings by Year in {round(frame_time,0)}\") +\n  labs(caption = \"Data from the IMDB DataSet\")\n\n animate(anim, renderer = gifski_renderer(file = paste0(output_dir, \"/animation1.gif\")))\n\n\n\n\n\n\n\n\n\nFrom this graph, we could identify that Dramas,Comedy,Action,Adventure, Crime have been always popular. However, there is a good amount attention being given for Biography and Animations in recent years.\n\n\n\nVI.Successful Personnel in the Genre\nBased on my findings I have decided to combine Biography Genre and Animation.\nWe focus on the Biography genre to identify high-quality films. The process begins by filtering the ALL_TITLES dataset to select only movies categorized under the Biography genre. The distinct function is used to ensure that each movie is represented only once in the resulting dataset.\n\n\nCode\nbio_anime_movies&lt;-ALL_TITLES|&gt;identify_title(\"movie\")|&gt;filter( genre_cleaned%in% c(\"Biography\",\"Animation\"))|&gt;distinct()\n\nbio_anime_movies &lt;- bio_anime_movies |&gt;filter(vote_quantile&gt;3)|&gt;select(-averageRating,-startYear,-X)|&gt;\n  left_join(ALL_CREW |&gt; filter(!is.na(primaryName)), by = c(\"tconst\" = \"tconst\")) |&gt;\n  distinct()\nsample_n(bio_anime_movies|&gt;arrange(desc(numVotes),desc(new_rating)),100)|&gt;DT::datatable()\n\n\n\n\n\n\n\nTask 5: Key Personnel\nIdentify (at least) two actors and one director who you will target as the key talent for your movie. Write a short “pitch” as to why they are likely to be successful. You should support your pitch with at least one graphic and one table.\nI chose Director as Brad Bird because of his extensive success records.\n\n\nCode\nfind_projects(\"Brad Bird\")|&gt;identify_title(\"movie\")|&gt; mutate(is_animation = ifelse(genre_cleaned %in% c(\"Animation\",\"Biography\"), 1, 0))|&gt;select(new_rating,numVotes,is_animation,genre_cleaned)|&gt;\n  ggplot(aes(x = new_rating, y = genre_cleaned, color = as.factor(is_animation))) +\n geom_boxplot() +  # Adjust shape for filled points\n  labs(\n    title = \"Brad Bird's Movies Rating Based on Genres\",\n    x = \"Ratings\",\n    y = \"Genre\",\n    color = \"Is_Animation\"\n  ) +\n  \n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nfind_projects(\"Brad Bird\")|&gt;identify_title(\"movie\")|&gt;select(-titleType,-tconst,-X)|&gt;filter(success_metrics==1)|&gt;DT::datatable()\n\n\n\n\n\n\n\n\n\nVII.Nostalgia and Remakes\nAfter reviewing the list of highly rated films on IMDb, one biographical movie stands out that I would love to remake: Lawrence of Arabia, originally released in 1962 and directed by David Lean. With an impressive 8.3 rating and over 321,000 votes.\n\n\nCode\ntable_creation(bio_anime_movies|&gt;filter(grepl(\"Lawrence of Arabia\", primaryTitle))|&gt;select(tconst,primaryTitle,numVotes,new_rating,decade)|&gt;distinct())\n\n\n\n\n\n\nThis film remains a masterpiece, telling the story of the war between Arab revolutionaries, British forces, and the Ottomans following World War I. While other films have touched on aspects of T.E. Lawrence’s life, none have revisited this pivotal narrative in a remake.\nI believe an animated version of this iconic story would be the perfect reimagining for several reasons. My inspiration comes largely from The Prophet, the animated adaptation of Khalil Gibran’s famous work. As a younger self, I found the animated format more digestible and memorable than simply reading through the poems. Animation has a unique ability to convey complex ideas and emotions in a way that is visually engaging and easier to absorb.\nSimilarly, I want today’s generation to experience this historical epic in a format that is accessible and captivating, without the need to sit through a lengthy three-hour film. By bringing this story to life through animation, the film would have a broader reach, appealing not just to movie critics but to a general audience who may find an animated version more approachable and engaging. It’s about creating a modern, digestible, and visually stunning way to explore a timeless historical narrative.\nThe voices would be delivered by Benedict Cumberbatch as T.E. Lawrence, Anthony Hopkins as Prince Faisal, and Morgan Freeman as the narrator. Directed by Brad Bird, known for The Incredibles, this project could introduce Lawrence of Arabia to a new generation while preserving its legendary status.\nThe original film featured a phenomenal creative team, including David Lean (Director), Robert Bolt and Michael Wilson (Writers), Peter O’Toole (T.E. Lawrence), and Alec Guinness (Prince Faisal). Unfortunately, all the original team is no longer with us, as per IMDb records, but their legacy would live on in this re imagined animated version.\n\n\nCode\n ALL_CREW |&gt;\n  filter(tconst == \"tt0056172\")|&gt;select(nconst,primaryName)|&gt;left_join(NAME_BASICS|&gt;select(nconst,primaryName,birthYear,deathYear),,by=c(\"nconst\"=\"nconst\",\"primaryName\"=\"primaryName\"))|&gt;select(-nconst)|&gt;arrange(desc(deathYear))|&gt;DT::datatable(caption = \"Lawence Of Arabia Crew\")\n\n\n\n\n\n\n\n\nVIII.Movie Pitch\n\n\n\ncredits:Tumblar\n\n\nIn analyzing trends of the popularity biographical and animated films have shown remarkable growth.They consistanly has average rating more than 6.5 every decade from 1940-now. Animation’s wide appeal across age groups—from children to adults—stems from its ability to highlight the beauty of everyday moments, turning them into deeply emotional and universally relatable stories. The format’s timeless appeal makes it perfect for projects that require both visual grandeur and intimate emotional depth.\nAn animated Lawrence of Arabia with Pixar Animation Studios would blend the sweeping beauty of desert landscapes and Bedouin culture with the raw emotions of war, betrayal, and the human experience. With the rich, resonant voices of Morgan Freeman, Benedict Cumberbatch, and Anthony Hopkins, the film could stir powerful emotions and empathy in viewers. And under the direction of Brad Bird, known for The Incredibles and Wall-E, this project could win hearts worldwide.\n\n\nAppendix\nCheck out my reviews and ratings for other movies. MY IMDB PROFILE"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "The U.S. Electoral College system assigns electoral votes to presidential candidates during election years. Although the details have evolved over time, some core principles remain unchanged. Each state is allotted R + 2 electoral votes (ECVs), where R represents the number of House Representatives the state has. States have the freedom to decide how to distribute their votes, and the candidate who secures a majority of the ECVs wins the presidency.\nThe Constitution does not mandate how states must allocate their ECVs, leaving this decision to each state. Historically, states have implemented various methods for distributing electoral votes, including:\n\nStatewide Winner-Take-All\nDistrict-Wide Winner-Take-All with Statewide “At Large” Votes\nStatewide Proportional\nNational Proportional\n\nThis project aims to analyze how U.S. presidential election outcomes would differ under various ECV allocation methods. This will involve examining historical congressional election data available online."
  },
  {
    "objectID": "mp03.html#i.introduction",
    "href": "mp03.html#i.introduction",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "The U.S. Electoral College system assigns electoral votes to presidential candidates during election years. Although the details have evolved over time, some core principles remain unchanged. Each state is allotted R + 2 electoral votes (ECVs), where R represents the number of House Representatives the state has. States have the freedom to decide how to distribute their votes, and the candidate who secures a majority of the ECVs wins the presidency.\nThe Constitution does not mandate how states must allocate their ECVs, leaving this decision to each state. Historically, states have implemented various methods for distributing electoral votes, including:\n\nStatewide Winner-Take-All\nDistrict-Wide Winner-Take-All with Statewide “At Large” Votes\nStatewide Proportional\nNational Proportional\n\nThis project aims to analyze how U.S. presidential election outcomes would differ under various ECV allocation methods. This will involve examining historical congressional election data available online."
  },
  {
    "objectID": "mp03.html#ii.set-up-and-initial-exploration",
    "href": "mp03.html#ii.set-up-and-initial-exploration",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "II.Set-Up and Initial Exploration",
    "text": "II.Set-Up and Initial Exploration\n\nLoading necessary packages\n\n\nCode\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"DT\")) install.packages(\"DT\")\nif (!require(\"sf\")) install.packages(\"sf\")\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif (!require(\"maps\")) install.packages(\"maps\")\nif (!require(\"gganimate\")) install.packages(\"gganimate\")\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(maps)\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(gganimate)\n\n\n\n\nLoading DataSet\n\nData I: US House Election Votes and Presidential Election Votes from 1976 to 2022\nThe MIT Election Data Science Lab collects votes from all biennial congressional races in all 50 states.The US President Election and House Of Representative Elections Dataset are downloaded.[^1] [^1]:MIT Election Data and Science Lab, 2017, “U.S. House 1976–2022”, https://doi.org/10.7910/DVN/IG0UN2, Harvard Dataverse, V13, UNF:6 /IVldA== [fileUNF]\n\ninitial exploration of the datasets\n\nUS HOUSE ELECTION DATSET\n\n\nCode\nskimr::skim(US_HOUSE_ELECTION_VOTES)\n\n\n\nData summary\n\n\nName\nUS_HOUSE_ELECTION_VOTES\n\n\nNumber of rows\n32452\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n7\n\n\nlogical\n5\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nstate\n0\n1\n4\n20\n0\n51\n0\n\n\nstate_po\n0\n1\n2\n2\n0\n51\n0\n\n\noffice\n0\n1\n8\n8\n0\n1\n0\n\n\nstage\n0\n1\n3\n3\n0\n2\n0\n\n\ncandidate\n0\n1\n4\n37\n0\n16231\n0\n\n\nparty\n0\n1\n0\n47\n3858\n478\n0\n\n\nmode\n0\n1\n5\n5\n0\n1\n0\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nrunoff\n8656\n0.73\n0.00\nFAL: 23788, TRU: 8\n\n\nspecial\n0\n1.00\n0.00\nFAL: 32362, TRU: 90\n\n\nwritein\n0\n1.00\n0.08\nFAL: 29722, TRU: 2730\n\n\nunofficial\n0\n1.00\n0.00\nFAL: 32413, TRU: 39\n\n\nfusion_ticket\n0\n1.00\n0.08\nFAL: 29812, TRU: 2640\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n1999.74\n13.63\n1976\n1988\n2000.0\n2012.0\n2022\n▇▇▇▇▇\n\n\nstate_fips\n0\n1\n28.76\n15.22\n1\n17\n31.0\n40.0\n56\n▆▅▆▇▅\n\n\nstate_cen\n0\n1\n50.95\n26.30\n11\n23\n52.0\n74.0\n95\n▇▇▆▅▆\n\n\nstate_ic\n0\n1\n37.09\n21.79\n1\n14\n40.0\n52.0\n82\n▇▆▇▃▅\n\n\ndistrict\n0\n1\n9.85\n9.97\n0\n3\n6.0\n13.0\n53\n▇▂▁▁▁\n\n\ncandidatevotes\n0\n1\n66763.39\n64531.40\n-1\n4324\n57328.5\n112143.8\n387109\n▇▅▁▁▁\n\n\ntotalvotes\n0\n1\n214807.50\n76503.36\n-1\n162266\n206983.0\n263386.0\n601509\n▁▇▅▁▁\n\n\nversion\n0\n1\n20230706.00\n0.00\n20230706\n20230706\n20230706.0\n20230706.0\n20230706\n▁▁▇▁▁\n\n\n\n\n\n\n\nPRESIDENTIAL VOTES DATASET\n\n\nCode\nskimr::skim(PRESIDENTIAL_VOTES)\n\n\n\nData summary\n\n\nName\nPRESIDENTIAL_VOTES\n\n\nNumber of rows\n4287\n\n\nNumber of columns\n15\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nlogical\n2\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nstate\n0\n1\n4\n20\n0\n51\n0\n\n\nstate_po\n0\n1\n2\n2\n0\n51\n0\n\n\noffice\n0\n1\n12\n12\n0\n1\n0\n\n\ncandidate\n0\n1\n0\n38\n287\n270\n0\n\n\nparty_detailed\n0\n1\n0\n40\n456\n173\n0\n\n\nparty_simplified\n0\n1\n5\n11\n0\n4\n0\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nwritein\n3\n1\n0.11\nFAL: 3807, TRU: 477\n\n\nnotes\n4287\n0\nNaN\n:\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n1999.08\n14.22\n1976\n1988\n2000\n2012.0\n2020\n▇▅▅▅▇\n\n\nstate_fips\n0\n1\n28.62\n15.62\n1\n16\n28\n41.0\n56\n▇▇▇▇▇\n\n\nstate_cen\n0\n1\n53.67\n26.03\n11\n33\n53\n81.0\n95\n▆▆▇▃▇\n\n\nstate_ic\n0\n1\n39.75\n22.77\n1\n22\n42\n61.0\n82\n▇▆▇▇▅\n\n\ncandidatevotes\n0\n1\n311907.59\n764801.10\n0\n1177\n7499\n199241.5\n11110250\n▇▁▁▁▁\n\n\ntotalvotes\n0\n1\n2366924.15\n2465008.31\n123574\n652274\n1569180\n3033118.0\n17500881\n▇▂▁▁▁\n\n\nversion\n0\n1\n20210113.00\n0.00\n20210113\n20210113\n20210113\n20210113.0\n20210113\n▁▁▇▁▁\n\n\n\n\n\n\n\n\n\nData II: Congressional Boundary Files\n\n\n1.Congression Boundaries\nJeffrey B. Lewis, Brandon DeVine, Lincoln Pritcher, and Kenneth C. Martis have created shapefiles for all US congressional districts from 1789 to 2012; I downloaded those shapefiles from 1976 to 2012 using get_data function using base URL.\n\ni.get_data Function\n\n\nCode\nget_data&lt;-function(destfile_dir){\n  BASE_URL&lt;-\"https://cdmaps.polisci.ucla.edu/shp/\"\nfile_names &lt;- sprintf(\"districts%03d.zip\", 95:112)\n\n\n   sf_list &lt;- list()\n for (file_name in file_names) {\n    destfile_path &lt;- file.path(destfile_dir, file_name)\n    \n    # Check if the file already exists\n    if (!file.exists(destfile_path)) {\n      # Construct the full URL\n      FILE_URL &lt;- paste0(BASE_URL, file_name)\n      \n      # Download the ZIP file\n      download.file(FILE_URL, destfile = destfile_path, mode = \"wb\")\n    }\n    \n   \n  }\n  \n  return(sf_list)\n}\n\n\n\n\nCode\nget_data(dest_dir)\n\n\nlist()"
  },
  {
    "objectID": "mp03.html#iii.initial-exploration-of-vote-count-data",
    "href": "mp03.html#iii.initial-exploration-of-vote-count-data",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "III.Initial Exploration of Vote Count Data",
    "text": "III.Initial Exploration of Vote Count Data\nNow that all the necessary data has been imported and ready to use, we can begin exploring the information we have available to us. The following questions can be answered by exploring the data.\n\n1. Which states have gained and lost the most seats in the US House of Representatives between 1976 and 2022?\n\n\nCode\nhead(US_HOUSE_ELECTION_VOTES)|&gt;DT::datatable()\n\n\n\n\n\n\n\nSubsetting US HOUSE ELECTION FOR YEAR 1976 and 2022\n\n\nCode\nelection_1976&lt;-US_HOUSE_ELECTION_VOTES|&gt;filter(year==1976)|&gt;group_by(state)|&gt;\n  summarise(seat_count_1976 = n_distinct(district)+2)\n\nelection_2022&lt;-US_HOUSE_ELECTION_VOTES|&gt;filter(year==2022)|&gt;group_by(state) %&gt;%\n  summarise(seat_count_2022 = n_distinct(district)+2)\n\n#calculate the seat Changes\nseat_changes&lt;-election_1976|&gt;left_join(election_2022,by=c(\"state\"=\"state\"))|&gt;mutate(change=seat_count_2022-seat_count_1976,state=tolower(state))|&gt;arrange(change)\n\n\n\n\nCode\nseat_changes|&gt;select(state,seat_count_1976,seat_count_2022,change)|&gt;\n  arrange(change)|&gt;slice(1:5)|&gt;DT::datatable(caption = \" Top 5 States that lost Seats\")\n\n\n\n\n\n\n\n\nCode\nseat_changes|&gt;select(state,seat_count_1976,seat_count_2022,change)|&gt;\n  arrange(desc(change))|&gt;slice(1:5)|&gt;DT::datatable(caption = \"Top 5 States that gained seats\")\n\n\n\n\n\n\n\n\nCode\n#the center of the map is converted to a df\nstate_centroids &lt;- data.frame(state = tolower(state.name), center.x = state.center$x, center.y = state.center$y,area=state.area)\n#join the center point df and the seat_changes\nseat_changes&lt;-seat_changes|&gt;inner_join(state_centroids,by=\"state\")\n\n ggplot(seat_changes|&gt;filter(change!=0), aes(x = center.x, y = center.y)) + \n          geom_polygon(data = map_data(\"state\"),\n                       aes(x = long, y = lat, group = group),\n                      color = \"white\",fill = \"grey90\")+\n      geom_label(aes(label = change,fill=change)) +\n   scale_fill_gradient2(name = \"Seat Change\", low = \"red\", mid = \"grey90\", high = \"green\", midpoint = 0) +\n   ggtitle(\"Changes in US House Seats by State (1976 and 2022)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n2. Could Fusion Voting System Have Altered Election Outcomes?\n\nElectoral fusion in the United States is an arrangement where two or more United States political parties on a ballot list the same candidate, allowing that candidate to receive votes on multiple party lines in the same election.\nElectoral fusion is also known as fusion voting, cross endorsement, multiple party nomination, multi-party nomination, plural nomination, and ballot freedom.\nElectoral fusion was once widespread in the U.S. and legal in every state. However, as of 2024, it remains legal and common only in New York and Connecticut.\nLets check whether there are any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes their received from their “major party line” (Democrat or Republican) and not their total number of votes across all lines.\n\nThe first line creates house_election_winner by starting with US_HOUSE_ELECTION_VOTES, then: - Groups the data by year, state,candidate.\nAdds a new column total_candidate_votes that calculates the total votes each candidate received across all party lines if fusion ticket=TRUE else keep the candidate votes as is for the total_candidate_votes column.\nSelects only relevant columns: year, state, district, candidate, party and total_candidate_votes.\n\n\n\nCode\n#### i. Calculate Total Votes per Candidate based on FUSION TICKET status:\\ \nhouse_election_filtered &lt;- US_HOUSE_ELECTION_VOTES |&gt;\n  group_by(year, state, candidate) |&gt;\n  mutate(total_candidate_votes = ifelse(fusion_ticket == \"TRUE\", sum(candidatevotes), candidatevotes)) |&gt;\n  select(year, state, district, candidate, party, candidatevotes, total_candidate_votes,totalvotes)\n#### ii. Results based on fusion ticket status is not taken into consideration\n\nno_fusion_impacted_results&lt;-house_election_filtered|&gt;filter(party %in% c(\"DEMOCRAT\",\"REPUBLICAN\"))|&gt; group_by(year,state,district)|&gt;slice_max(order_by=candidatevotes,n=1)|&gt;rename(candidate_winner_by_party=candidate,represented_party=party)|&gt;select(-total_candidate_votes)\n#### iii. Results based on fusion ticket status is taken into consideration\n\nfusion_impacted_results&lt;-house_election_filtered|&gt;filter(party %in% c(\"DEMOCRAT\",\"REPUBLICAN\"))|&gt; group_by(year,state,district)|&gt;slice_max(order_by=total_candidate_votes,n=1)|&gt;rename(orginal_winner=candidate,main_party=party)|&gt;select(-candidatevotes)\n###result change while comparing both\nelection_results_change &lt;- no_fusion_impacted_results |&gt;\n  left_join(fusion_impacted_results, by = c(\"year\", \"state\", \"district\")) |&gt;\n  mutate(different_outcome = total_candidate_votes != candidatevotes) |&gt;  # TRUE if total votes differ from candidate votes\n  filter(different_outcome == TRUE & candidate_winner_by_party != orginal_winner) |&gt;  # Only keep rows where outcomes differ and winners are different\n  select(-different_outcome)\n\nelection_results_change|&gt;DT::datatable()\n\n\n\n\n\n\nWhen we check if the original winner and candidate winner by party are same, it is FALSE, indicating both are not same.\n\n\nCode\nall(election_results_change$orginal_winner == election_results_change$candidate_winner_by_party)\n\n\n[1] FALSE\n\n\nThe results would have been different if fusion vote system was not introduced for HOUSE ELECTIONS. But for the Presidential Election, it does not matter.\n\n\n3. Do Presidential Candidates Run Ahead of or Behind Congressional Candidates in the Same State?\n\n\nCode\nhouse_election_party_votes&lt;-house_election_filtered|&gt;filter(party %in% c(\"DEMOCRAT\",\"REPUBLICAN\"))|&gt;group_by(year,state,party)|&gt;summarize(total_house_party_votes=sum(total_candidate_votes),.groups=\"drop\")\n\npresidential_party_votes &lt;- PRESIDENTIAL_VOTES |&gt;\n  group_by(year, state,party_simplified) |&gt;\n  summarize(total_presidential_party_votes = sum(candidatevotes, na.rm = TRUE), .groups = \"drop\") |&gt;filter(party_simplified!=\"OTHER\")\n\nvote_disparity &lt;- presidential_party_votes |&gt;\n  inner_join(house_election_party_votes, by = c(\"year\", \"state\", \"party_simplified\"=\"party\")) \n\nvote_difference=vote_disparity|&gt;mutate(vote_difference = total_presidential_party_votes - total_house_party_votes) |&gt;\n  select(-total_presidential_party_votes, -total_house_party_votes)\n\n# Assuming your data frame is called 'df'\nvotes_join &lt;- vote_disparity %&gt;%\n  pivot_longer(cols = c(total_presidential_party_votes, total_house_party_votes),\n               names_to = \"vote_type\", values_to = \"total_votes\")\n\nvotes_join |&gt;\n  group_by(year, vote_type, party_simplified) |&gt;\n  summarize(total_votes_by_year = sum(total_votes), .groups = \"drop\") |&gt;\n  ggplot(aes(x = year, y = log10(total_votes_by_year), group = vote_type)) +  # Log-transform the y-axis directly in `aes`\n  geom_point(aes(color = vote_type)) +\n  geom_line(aes(color=vote_type)) +\n  # Add vote difference\n  scale_color_manual(\n    values = c(\"total_presidential_party_votes\" = \"green\", \"total_house_party_votes\" = \"orange\"),\n    labels = c(\"total_presidential_party_votes\" = \"Presidential Election\", \"total_house_party_votes\" = \"House Election\")\n  ) +\n  labs(\n    title = \"Total Votes Over Time by Party\",\n    x = \"Year\",\n    y = \"Log10 Transformed Total Votes\",\n    color = \"Election Type\"  # Custom title for the legend\n  ) +\n  facet_wrap(~party_simplified) +\n  theme_minimal() +  # Cleaner theme\n  theme(\n    legend.position = \"bottom\",  # Move legend to bottom for better aesthetics\n    axis.text.x = element_text(angle = 45, hjust = 1)  # Tilt x-axis labels for better readability\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(scales)\nvote_difference=vote_difference|&gt;left_join(PRESIDENTIAL_VOTES|&gt;select(year,state,state_po),by=c(\"year\",\"state\"))\n\n# Assuming your data frame is called `vote_difference_df`\nggplot(\n  vote_difference |&gt; filter(vote_difference &lt; 0)|&gt;arrange(state_po), \n  aes(x = year, y = state_po, fill = vote_difference)\n) +\n  geom_tile(color = \"black\") +\n  scale_fill_gradient2(\n    low = \"green\", mid = \"white\", high = \"red\", midpoint = 0,\n    name = \"Vote Difference\",\n    labels = label_comma()  # Formats the legend labels with commas\n  ) +\n  labs(\n    title = \"Heatmap of Vote Difference where Presidential Candidate\\nHad More Votes than All US House Election Candidates\\nby State and Party\",\n    x = \"Year\",\n    y = \"State\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    axis.text.y = element_text(size = 4),  # Adjusted size for better visibility\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),  # Center-aligned and bold title\n    legend.position = \"bottom\",  # Moves the legend to the bottom\n    legend.title = element_text(size = 10),\n    legend.text = element_text(angle=45,size = 4)\n  ) +\n  facet_wrap(~party_simplified, scales = \"free_y\")  # Adjusts y-scale per facet for better display\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Assuming your data frame is called `vote_difference_df`\nggplot(\n  vote_difference |&gt; filter(vote_difference &gt; 0)|&gt;arrange(state_po)|&gt;mutate(state_po=tolower(state_po)), \n  aes(x = year, y = state_po, fill = vote_difference)\n) +\n  geom_tile(color = \"black\") +\n  scale_fill_gradient2(\n    low = \"green\", mid = \"white\", high = \"red\", midpoint = 0,\n    name = \"Vote Difference\",\n    labels = label_comma()  # Formats the legend labels with commas\n  ) +\n  labs(\n    title = \"Heatmap of Vote Difference where Presidential Candidate\\nHad LESS Votes than All US House Election Candidates\\nby State and Party\",\n    x = \"Year\",\n    y = \"State\"\n  ) +\n    theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    axis.text.y = element_text(size = 4.5),  # Adjusted size for better visibility\n    plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),  # Center-aligned and bold title\n    legend.position = \"bottom\",  # Moves the legend to the bottom\n    legend.title = element_text(size = 10),\n    legend.text = element_text(angle=45,size = 4)\n  ) +\n  facet_wrap(~party_simplified, scales = \"free_y\")  # Adjusts y-scale per facet for better display"
  },
  {
    "objectID": "mp03.html#iii.-importing-and-plotting-shape-file-data",
    "href": "mp03.html#iii.-importing-and-plotting-shape-file-data",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "III. Importing and Plotting Shape File Data",
    "text": "III. Importing and Plotting Shape File Data\nEarlier we downloaded zip files for the US congressional districts. In order to access the shapefiles, we can automate a zip file extraction with some code using read_shp_from_zip function.\n\ni.read_shp_from_zip Function\n\n\nCode\n#Task 4\nread_shp_from_zip&lt;-function(zippedURL)\n  {\n  td &lt;- tempdir(); \nzip_contents &lt;- unzip(zippedURL, \n                      exdir = td)\n    \nfname_shp &lt;- zip_contents[grepl(\"\\\\.shp$\", zip_contents)]\nboundaries_sf &lt;- read_sf(fname_shp)\nreturn(boundaries_sf)\n}\n\n\nusing con_boundaries FUNCTION function, I read the .shp files from the zip content and are stored as a list. #### ii.con_boundaries FUNCTION\n\n\nCode\n#|label: 'con_boundaries function'\n#|warning: false\ncon_boundaries&lt;-function(dest_dir){\n\nfile_names &lt;- sprintf(\"districts%03d.zip\", 95:112)\nsf_list &lt;- list()\nnum&lt;-95\nfor (file_name in file_names) {\ndestfile_path &lt;- file.path(dest_dir, file_name)\n  \n# Read the shape file and store in the list\n   sf_object &lt;- read_shp_from_zip(destfile_path)\n    name&lt;-paste0(\"cd\",num)\n  sf_list[[name]] &lt;- sf_object\n  num&lt;-num+1\n}\nreturn(sf_list)\n}\n\nUS_CONGRESSIONAL_BOUNDARIES1&lt;-con_boundaries(dest_dir)\n\n\n\n\nCode\nsummary(US_CONGRESSIONAL_BOUNDARIES1)\n\n\n      Length Class Mode\ncd95  16     sf    list\ncd96  16     sf    list\ncd97  16     sf    list\ncd98  16     sf    list\ncd99  16     sf    list\ncd100 16     sf    list\ncd101 16     sf    list\ncd102 16     sf    list\ncd103 16     sf    list\ncd104 16     sf    list\ncd105 16     sf    list\ncd106 16     sf    list\ncd107 16     sf    list\ncd108 16     sf    list\ncd109 16     sf    list\ncd110 16     sf    list\ncd111 16     sf    list\ncd112 16     sf    list\n\n\n\n\nChloropleth Maps\nTo get a better idea of what can be done with the shape files, a chloropleth visualization of the electoral college results for the 2000 presidential election will be created.\n\ni.Subsetting PRESIDENTIAL VOTES and HOUSE ELECTION VOTES FOR YEAR 2000 and Finding the Winners\n\n\nCode\n#subsetting presidential vote for year 2000\npresidential_votes2000&lt;-PRESIDENTIAL_VOTES|&gt;filter(year==2000)|&gt;group_by(state)|&gt;slice_max(candidatevotes)\n\n\n#subsetting US HOUSE ELECTION votes for year 2000 and keeping only Democrat and Republican Party\nelection_2000_winner&lt;-fusion_impacted_results|&gt;filter(year==2000)\n\n#finding the total seats\ntotal_seats&lt;-election_2000_winner|&gt;select(state,district)|&gt;group_by(state)|&gt;mutate(no_of_seats=n_distinct(district)+2)|&gt;select(state,no_of_seats)|&gt;unique()\n\n#finding 2000 presidential election winner for each state \npresidential_votes2000&lt;-presidential_votes2000|&gt;group_by(state,candidate)|&gt;slice_max(candidatevotes)|&gt;select(year,state,candidate,party_simplified,candidatevotes,totalvotes)|&gt;\n  left_join(total_seats,by=c(\"state\"=\"state\"))\n\n\n\n\nii.Choosing District Shape File\n\n\nCode\n#106th congress district shape files are selected.\ndistrict_shapes2000&lt;-US_CONGRESSIONAL_BOUNDARIES1$cd106|&gt;select(STATENAME,ID, DISTRICT,geometry)\n#validating the geometry of district shape file and finding the center of each state\ndistrict_shapes2000&lt;-district_shapes2000|&gt;mutate(geometry = st_make_valid(geometry),\nstate_centroid = st_centroid(geometry),\n    centroid_x = st_coordinates(state_centroid)[, 1],  # Extract x-coordinates\n    centroid_y = st_coordinates(state_centroid)[, 2]   # Extract y-coordinates\n  )\n#another method to get the us state map\nus_map=map_data(\"state\")\nus_map=us_map|&gt;inner_join(state_centroids,by=c(\"region\"=\"state\"))\n#unionizing the geometry from district_shape file to make it for state boundaries\n#there are other libraries and other state boundary shp's but I just gave a try this way!!\nstates_shapes2000&lt;-district_shapes2000|&gt;select(STATENAME,geometry)|&gt;\n  group_by(STATENAME)|&gt;\n  summarize(geometry = st_union(geometry))|&gt;\n  filter(!is.na(geometry))\n\n\n\n\n\niii. Joining election results and shapefiles\n\n\nCode\n#joining 2000 congressional district election result and district shapefile\nelection_2000_winner&lt;-election_2000_winner|&gt;select(state,district,main_party,total_candidate_votes,totalvotes)|&gt;mutate(state=tolower(state),district=as.character(district))|&gt;left_join(district_shapes2000|&gt;mutate(STATENAME=tolower(STATENAME)),join_by(\"state\"==\"STATENAME\",\"district\"==\"DISTRICT\"))\n\n\n\n#joining the state boundaries and presidential vote results for year 2000\nus_states_2000&lt;-states_shapes2000|&gt;mutate(STATENAME=tolower(STATENAME))|&gt;left_join(presidential_votes2000|&gt;select(state,party_simplified,candidate)|&gt;mutate(state=tolower(state)),by=c(\"STATENAME\"=\"state\"))|&gt;unique()\n\n#to find the centeroid of each state\nus_states_2000&lt;-us_states_2000|&gt;filter(!is.na(geometry), !is.na(candidate))|&gt;left_join(total_seats,by=c(\"STATENAME\"=\"state\"))\n#converting to sf\nus_states_2000&lt;-st_as_sf(us_states_2000)\n#coverting the geometry to sf\nelection_2000_winner_sf &lt;- st_as_sf(election_2000_winner)  \n\n\n\n\nElection results by Congressional district 2000\n\niv. Interactive Map to Include Alaska and Hawaii for Congressional District Win\n\n\nCode\nlibrary(leaflet)\nparty_colors &lt;- colorFactor(palette = c(\"blue\", \"red\",\"white\"), \n                            levels = c(\"DEMOCRAT\", \"REPUBLICAN\",\"OTHER\"))\n\n# Create the leaflet map\nleaflet(election_2000_winner_sf) |&gt;\n  addTiles() |&gt;\n  addPolygons(fillColor = ~party_colors(main_party), # Apply color based on party\n              color = \"black\", \n              weight = 1,\n              fillOpacity = 0.5,\n              popup = ~paste(\"State:\", state,\"District:\",district))|&gt;\n  addLegend(\n    pal = party_colors,\n    values = ~main_party,\n    title = \"Winning Party By Congressional District 2000\",\n    position = \"bottomright\"\n  )\n\n\n\n\n\n\n\n\nv. Election results by Congressional district 2000 Static\n\n\nCode\n# Static Plot for the 2000 election result\nggplot(election_2000_winner_sf) +\n  geom_sf(aes(fill = main_party), color = \"white\") +\n #geom_text(aes(x = centroid_x, y = centroid_y, label = state), size = 3, color = \"black\") +\n  scale_fill_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\")) +\n  coord_sf(xlim = c(-130, -60), ylim = c(24, 50))  +\n  labs(fill = \"Winning Party\",\n       title = \"Election results by Congressional district 2000\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n2000 Electoral College Map By State\n\nvi. Interactive map to include Alaska and Hawaii\n\n\nCode\nlibrary(sf)\nlibrary(dplyr)\n\n# Filter and convert unsupported geometries\nus_states_2000 &lt;- us_states_2000 %&gt;%\n  filter(!is.na(geometry), !is.na(party_simplified)) %&gt;%\n  st_make_valid() %&gt;%  # Ensure geometries are valid\n  st_cast(\"MULTIPOLYGON\")  # Convert to MULTIPOLYGON if needed\n#st_write(us_states_sf,\"us_states_sf.shp\")\n\nleaflet(us_states_2000) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~party_colors(party_simplified),\n    color = \"black\",\n    weight = 1,\n    fillOpacity = 0.5,\n    label = ~paste(\"State:\", STATENAME, \"No. of Seats:\", no_of_seats),\n    highlightOptions = highlightOptions(\n      weight = 2,\n      color = \"blue\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE\n    ))|&gt;\n  addLegend(\n    pal = party_colors,\n    values = ~party_simplified,\n    title = \"2000 Electoral College Map Winning Party By State\",\n    position = \"bottomright\"\n  )\n\n\n\n\n\n\n\n\nvii. Electoral College Map For year 2000\n\n\nCode\nus_state_static&lt;-presidential_votes2000|&gt;select(year,state,party_simplified,no_of_seats)|&gt;left_join(us_map|&gt;mutate(STATENAME=toupper(region)),by=c(\"state\"=\"STATENAME\"))\n# Plot the data\nggplot(us_state_static) +\n  geom_polygon(aes(x=long,y=lat,group=group, fill = party_simplified), color = \"white\")+\n  geom_text(aes(x = center.x, y = center.y, label = no_of_seats), size = 3, color = \"black\") +\n  scale_fill_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\")) +\n  coord_sf(xlim = c(-130, -60), ylim = c(24, 50))  +\n  labs(fill = \"Winning Party\",\n       title=\"Electoral College Map 2000\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "mp03.html#iv.-election-results-over-the-time",
    "href": "mp03.html#iv.-election-results-over-the-time",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "IV. Election Results Over The Time",
    "text": "IV. Election Results Over The Time\n\n\nCode\n#presidential winner by state for each year from 1976 to 2020\npresidential_winners_by_state&lt;-PRESIDENTIAL_VOTES|&gt;select(year,state,candidatevotes,candidate,party_simplified)|&gt;\n  group_by(year,state)|&gt;\n  slice_max(candidatevotes)\n\n#combining the presidential winner by state for each year with us states boundary sf\npresidentials_winners_combined_sf&lt;-presidential_winners_by_state|&gt;select(year,state,party_simplified)|&gt;left_join(us_map|&gt;mutate(STATENAME=toupper(region)),by=c(\"state\"=\"STATENAME\"))\n#creating a facet wrap map for the results \nggplot(presidentials_winners_combined_sf) +\n  geom_polygon(aes(x=long,y=lat,group=group, fill = party_simplified), color = \"white\") +\n  scale_fill_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\")) +\n  coord_sf(xlim = c(-130, -60), ylim = c(24, 50))  +\n  labs(fill = \"Winning Party\") +\n  theme_minimal() +\ntheme(\n  legend.position = \"bottom\",\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank()\n  )+\nfacet_wrap(~year)  # `ncol = 2` ensures two maps per row\n\n\n\n\n\n\n\n\n\n\ni.State-Wide Winner-Take-All\n\n\nCode\n# Evaluating Fairness of ECV Allocation Schemes\n\n# Calculate electoral votes for each state by year (Reps + 2 votes for senators)\nvotes_per_state &lt;- US_HOUSE_ELECTION_VOTES |&gt;\n  group_by(year, state) |&gt;\n  summarize(electoral_college_votes = n_distinct(district) + 2, .groups = \"drop\") \n\n# Add electoral votes for the District of Columbia for the years 1976-2020\ndc_votes &lt;- tibble(\n  year = rep(1976:2020, each = 1),\n  state = rep(\"DISTRICT OF COLUMBIA\", 45),\n  electoral_college_votes = 3\n)\n\n# Combine the DC data with the rest of the states' data\nelectoral_votes_per_state &lt;- bind_rows(votes_per_state, dc_votes)\n\n# Identify the winning candidate for each state and year\nstate_winner_results &lt;- PRESIDENTIAL_VOTES |&gt;\n  group_by(year, state, candidate) |&gt;\n  summarize(total_votes = sum(candidatevotes), \n            total_votes_year = sum(totalvotes, na.rm = TRUE),\n              .groups = \"drop\", ) |&gt;\n  mutate(popularity_per_state_percentage = \n           (total_votes / total_votes_year) * 100)|&gt;\n  group_by(year, state) |&gt;\n  slice_max(order_by = total_votes, n = 1, with_ties = FALSE) |&gt;\n  rename(winning_candidate = candidate)\n\n# Merge with electoral votes data and calculate the total electoral votes for each candidate\nstate_winner_by_count &lt;- state_winner_results |&gt;\n  left_join(electoral_votes_per_state, by = c(\"year\", \"state\"))|&gt;\n  group_by(year, winning_candidate) |&gt;\n  summarize(total_electoral_votes = sum(electoral_college_votes))|&gt;slice_max(order_by = total_electoral_votes, n = 1, with_ties = FALSE)|&gt;ungroup()\n\n# Add party information and remove duplicates\ncandidate_party &lt;- PRESIDENTIAL_VOTES |&gt; select(year, candidate, party_simplified)\n\nstate_winner_by_count &lt;- state_winner_by_count |&gt;\n  left_join(candidate_party|&gt;filter(party_simplified %in% c(\"DEMOCRAT\",\"REPUBLICAN\")), by = c(\"year\" = \"year\", \"winning_candidate\" = \"candidate\")) |&gt;\n  distinct()\n\n# Display results in a data table\ndatatable(\n  setNames(\n    state_winner_by_count |&gt; filter(party_simplified != \"OTHER\"), \n    c(\"Election Year\", \"Winning Candidate\", \"Electoral Votes\", \"Party\")\n  ),\n  options = list(pageLength = 12, autoWidth = TRUE),\n  caption = \"State-Wide Winner-Take-All: Presidential Winning Candidate\"\n)\n\n\n\n\n\n\n\n\nii.District-Wide Winner-Take-All + State-Wide “At Large” Votes\n\n\nCode\n# find number of districts each party won to represent electoral votes won in each state\ndistrict_winner &lt;- fusion_impacted_results |&gt;\n  group_by(year, state, district) |&gt;\n  slice_max(order_by = \n              total_candidate_votes, n = 1, with_ties = FALSE)|&gt;\n  select(year, state, district, main_party) |&gt;\n  group_by(year, state, main_party) |&gt;\n  summarize(districts_won = n()) # number of electoral votes received by each party\n\n# find popular vote winner in the state\nat_large_winner &lt;- fusion_impacted_results|&gt;\n  group_by(year, state) |&gt;\n  slice_max(order_by = total_candidate_votes, n = 1, with_ties = FALSE)|&gt;mutate(at_large_vote=1)\n\n#|&gt;\n# select(year, state, party) |&gt;\n#add_column(at_large_votes = 2) # designating the vote count\n\n# join tables together to find total electoral votes the presidential party receives in each state\n\ndistrict_wide_winner_take_all &lt;- district_winner |&gt;\n  left_join(at_large_winner|&gt;select(year,state,main_party,at_large_vote),\n    by = c(\"year\", \"state\", \"main_party\")\n  ) |&gt;\n  mutate(across(where(is.numeric), ~ ifelse(is.na(.), 0, .))) |&gt; # set NA to 0 for the rows that had no resulting joins\n  mutate(total_electoral_votes = districts_won + at_large_vote) |&gt;  select(-districts_won, -at_large_vote)|&gt; #join the Presidential_Votes\n  left_join(PRESIDENTIAL_VOTES,\n    by = c(\"year\", \"state\", \"main_party\"=\"party_simplified\")) |&gt; # join to presidential candidate\n  select(year, state, total_electoral_votes, candidate) |&gt;\n  group_by(year, candidate) |&gt;\n  summarize(electoral_votes = sum(total_electoral_votes)+1) |&gt;\n  drop_na() # get rid of the non-presidential election years\n\n\ndistrict_wide_winner_take_all&lt;-district_wide_winner_take_all|&gt;left_join(candidate_party,by=c(\"year\",\"candidate\"))|&gt;filter(party_simplified %in% c(\"DEMOCRAT\",\"REPUBLICAN\"))|&gt;unique()\ndatatable(setNames(district_wide_winner_take_all, c(\"Year\", \"Winning Candidate\", \"Electoral Votes\",\"PARTY\")),\n          options = list(pageLength = 12, autoWidth = TRUE),\n          caption = \"District-Wide Winner-Take-All: Presidential Winning Candidate\"\n)\n\n\n\n\n\n\n\n\niii.State-Wide Proportional\n\n\nCode\n# find the number of electoral votes received by each candidate\n\nstate_wide_proportional &lt;- state_winner_results |&gt;\n  left_join(electoral_votes_per_state,\n    by = c(\"year\", \"state\")\n  ) |&gt;  mutate(votes_received = round(popularity_per_state_percentage * electoral_college_votes, digits =2) )\n\n\n# sum total votes and find presidential winner\nstate_wide_proportional_winner &lt;- state_wide_proportional |&gt;\n  group_by(year, winning_candidate) |&gt;\n  summarize(total_electoral_votes = sum(votes_received)) |&gt;\n  slice_max(order_by = total_electoral_votes, n = 1, with_ties = FALSE) |&gt;\n  rename(winner = winning_candidate)\n\nstate_wide_proportional_winner&lt;-state_wide_proportional_winner|&gt;left_join(candidate_party, by=c(\"year\",\"winner\"=\"candidate\"))|&gt;unique()\n\nstate_wide_proportional_winner|&gt;filter(party_simplified!=\"OTHER\")|&gt;DT::datatable()\n\n\n\n\n\n\n\n\niv. Nation-Wide Proportional\n\n\nCode\n# find total number of electoral votes available\nelectoral_votes_available &lt;- electoral_votes_per_state |&gt;\n  group_by(year) |&gt;\n  summarize(total_college_votes = sum(electoral_college_votes))\n\n# find percentage of popular vote each candidate received\nnational_proportional &lt;-PRESIDENTIAL_VOTES |&gt;\n  select(year, state, candidate, candidatevotes) |&gt;\n  group_by(year, candidate) |&gt;\n  summarize(total_electoral_votes = sum(candidatevotes)) |&gt;\n  group_by(year) |&gt;\n  mutate(population_vote_count = sum(total_electoral_votes)) |&gt; # find total number of votes cast in election year\n  ungroup() |&gt;\n  mutate(percentage_population_vote = (total_electoral_votes / population_vote_count)) |&gt;\n  select(-total_electoral_votes, -population_vote_count) |&gt;\n  # find the proportion of the electoral votes received based on the popular vote percentage\n  left_join(\n    electoral_votes_available,\n    join_by(year == year)\n  ) |&gt;\n  mutate(electoral_votes_received = round(percentage_population_vote * total_college_votes, digits = 0)) |&gt;\n  select(-percentage_population_vote, -total_college_votes) |&gt;\n  group_by(year) |&gt;\n  slice_max(order_by = electoral_votes_received, n = 1, with_ties = FALSE) |&gt;\n  rename(winner = candidate)\n\ndatatable(setNames(national_proportional, c(\"Year\", \"Winning Candidate\", \"Electoral Votes\")),\n          options = list(pageLength = 12, autoWidth = TRUE),\n          caption = \"National Proportional: Presidential Winning Candidate\"\n)\n\n\n\n\n\n\nTo evaluate the fairness of different Electoral College Vote (ECV) allocation schemes, it’s essential to first define the characteristics that make a scheme “fair.” I believe that the national proportional allocation scheme is the most balanced and representative approach. This system allocates electoral votes in direct proportion to the popular vote nationwide, ensuring that every voter’s voice is considered equally. Since the U.S. president represents the entire country, it seems fair that the electoral process should reflect the national popular will. This method addresses the disparities in population distribution across states, allowing for equitable representation regardless of where voters live.\n\n\nAnalysis of ECV Allocation Schemes:\n\nNational Proportional Scheme:\n\nPros: Represents the entire nation’s voting preferences proportionally. It offers a comprehensive view of the electorate’s overall decision, minimizing regional biases and amplifying the collective voice.\nCons: Implementation may lead to coalition-building due to the potential for a non-majority winner, which some argue could create instability.\n\nState-Wide Proportional Scheme:\n\nPros: Reflects the diverse opinions within each state by allocating electoral votes based on state-level vote proportions. This method captures a more nuanced state voter base while preserving the state-focused nature of the Electoral College.\nCons: It still divides the country into state units, which may create disparities based on state-level populations and the distribution of electoral votes, skewing overall representation.\n\nState-Wide Winner-Take-All Scheme:\n\nPros: Simple and clear-cut, which contributes to decisive outcomes and preserves state influence in national elections.\nCons: Heavily distorts the national popular vote by awarding all electoral votes to the majority winner in each state. This can result in a president who did not win the popular vote, as seen in several U.S. elections.\n\nDistrict-Wide Winner-Take-All Scheme:\n\nPros: Reflects the choice of voters within smaller units (districts), giving localized representation and a more granular electoral outcome.\nCons: Can be heavily impacted by gerrymandering, where district boundaries are drawn strategically to favor a political party, leading to outcomes that don’t align with broader voter intentions.\n\n\n\n\nCase Study: The 2000 Presidential Election\nThe 2000 presidential election between George W. Bush and Al Gore is an excellent case where ECV allocation schemes had significant implications. Under the national proportional allocation, Al Gore would have won the election, as he secured a higher percentage of the popular vote. However, under the historical state-wide winner-take-all scheme and the district-wide scheme, George W. Bush won due to winning key states, even with a smaller national vote share.\nThis case underscores the impact of different allocation methods. The national proportional scheme would have shifted the outcome to align with the popular vote, highlighting its fairness in mirroring the nation’s collective decision. Other schemes resulted in disparities, showcasing how different strategies can shift the electoral outcome and potentially misrepresent the true will of the voters.\n\n\nConclusion\nEach electoral scheme offers unique benefits and drawbacks. While the national proportional scheme provides the most equitable reflection of national voter preferences, it may require adjustments to ensure clear governance in case of non-majority results. The state-wide proportional scheme offers a compromise, but still falls short of capturing the national perspective uniformly. The winner-take-all systems, whether state-wide or district-wide, are the least representative, as they amplify select majorities and can distort the overall outcome."
  },
  {
    "objectID": "mp03.html#v.extra-credit-opportunity",
    "href": "mp03.html#v.extra-credit-opportunity",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "V.Extra Credit Opportunity\n",
    "text": "V.Extra Credit Opportunity"
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "",
    "text": "In this project, I am tackling a vital personal financial decision using R: selecting a retirement plan as a newly hired faculty member at CUNY. Faculty members have 30 days to choose between two options, and this decision is essentially permanent. Choosing the right plan is no small task—it requires careful analysis of financial trends, demographic assumptions, and personal circumstances. Through this project, I plan to dive into historical financial data and use a bootstrap inference strategy to estimate the probability that one plan outperforms the other. CUNY offers two retirement plans, the traditional defined-benefit Teachers Retirement System (TRS) plan and the newer defined-contribution Optional Retirement Plan (ORP). For this project, I ignore the effect of taxes as both plans offer pre-tax retirement savings, so whichever plan has the greater (nominal, pre-tax) income will also yield the greater (post-tax) take-home amount.\n\n\n\nEmployees pay a fixed percentage of their paycheck into the pension fund. For CUNY employees joining after March 31, 2012–which you may assume for this project–the so-called “Tier VI” contribution rates are based on the employee’s annual salary and increase as follows:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\n\n\nThe retirement benefit is calculated based on the Final Average Salary of the employee: following 2024 law changes, the FAS is computed based on the final three years salary. (Previously, FAS was computed based on 5 years: since salaries tend to increase monotonically over time, this is a major win for TRS participants.)\nIf \\(N\\) is the number of years served, the annual retirement benefit is:\n\n\n\\(1.67\\% * \\text{FAS} * N\\) if \\(N \\leq 20\\)\n\n\n\\(1.75\\% * \\text{FAS} * N\\) if \\(N = 20\\)\n\n\n\\((35\\% + 2\\% * (N - 20)) * \\text{FAS}\\) if \\(N \\geq 20\\)[^3][^3b]\n\nIn each case, the benefit is paid out equally over 12 months.\n\n\nThe benefit is increased annually by 50% of the CPI, rounded up to the nearest tenth of a percent: e.g., a CPI of 2.9% gives an inflation adjustment of 1.5%. The benefit is capped below at 1% and above at 3%, so a CPI of 10% leads to a 3% inflation adjustment while a CPI of 0% leads to a 1% inflation adjustment.\nThe inflation adjustement is effective each September and the CPI used is the aggregate monthly CPI of the previous 12 months; so the September 2024 adjustment depends on the CPI from September 2023 to August 2024.\n\n\nThe ORP plan is more similar to a 401(k) plan offered by a private employer. The employee and the employer both make contributions to a retirement account which is then invested in the employee’s choice of mutual funds. Those investments grow “tax-free” until the employee begins to withdraw them upon retirement. If the employee does not use up the funds, they are passed down to that employee’s spouse, children, or other heirs; if the employee uses the funds too quickly and zeros out the account balance, no additional retirement funds are available. Though the employee hopefully still has Social Security retirement benefits and other savings to cover living expenses. This type of plan is called a defined-contribution plan as only the contributions to the retirement account are fixed by contract: the final balance depends on market factors outside of the employee’s control.\nAt retirement, the employee has access to those funds and can choose to withdraw them at any rate desired. A general rule of thumb is withdrawing 4% of the value per year, e.g., this Schwab discussion; you can assume a constant withdrawal rate in your analysis. Note that unwithdrawn funds continue to experience market returns.\nThe funds available in a ORP account depend strongly on the investments chosen. For this analysis, you can assume that the ORP participants invest in a Fidelity Freedom Fund with the following asset allocation:[^6]\n\nAge 25 to Age 49:\n\n54% US Equities\n36% International Equities\n10% Bonds\n\n\nAge 50 to Age 59:\n\n47% US Equities\n32% International Equities\n21% Bonds\n\n\nAge 60 to Age 74:\n\n34% US Equities\n23% International Equities\n43% Bonds\n\n\nAge 75 or older:\n\n19% US Equities\n13% International Equities\n62% Bonds\n6% Short-Term Debt\n\n\n\nUnder the ORP, both the employee and the employer make monthly contributions to the employee’s ORP account. These contributions are calculated as a percentage of the employee’s annual salary. Specifically, the employee contributes at the same rate as the TRS:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nThe employer contribution is fixed at:\n\n8% for the first seven years of employment at CUNY.\n10% for all years thereafter."
  },
  {
    "objectID": "mp04.html#introduction",
    "href": "mp04.html#introduction",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "",
    "text": "In this project, I am tackling a vital personal financial decision using R: selecting a retirement plan as a newly hired faculty member at CUNY. Faculty members have 30 days to choose between two options, and this decision is essentially permanent. Choosing the right plan is no small task—it requires careful analysis of financial trends, demographic assumptions, and personal circumstances. Through this project, I plan to dive into historical financial data and use a bootstrap inference strategy to estimate the probability that one plan outperforms the other. CUNY offers two retirement plans, the traditional defined-benefit Teachers Retirement System (TRS) plan and the newer defined-contribution Optional Retirement Plan (ORP). For this project, I ignore the effect of taxes as both plans offer pre-tax retirement savings, so whichever plan has the greater (nominal, pre-tax) income will also yield the greater (post-tax) take-home amount.\n\n\n\nEmployees pay a fixed percentage of their paycheck into the pension fund. For CUNY employees joining after March 31, 2012–which you may assume for this project–the so-called “Tier VI” contribution rates are based on the employee’s annual salary and increase as follows:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\n\n\nThe retirement benefit is calculated based on the Final Average Salary of the employee: following 2024 law changes, the FAS is computed based on the final three years salary. (Previously, FAS was computed based on 5 years: since salaries tend to increase monotonically over time, this is a major win for TRS participants.)\nIf \\(N\\) is the number of years served, the annual retirement benefit is:\n\n\n\\(1.67\\% * \\text{FAS} * N\\) if \\(N \\leq 20\\)\n\n\n\\(1.75\\% * \\text{FAS} * N\\) if \\(N = 20\\)\n\n\n\\((35\\% + 2\\% * (N - 20)) * \\text{FAS}\\) if \\(N \\geq 20\\)[^3][^3b]\n\nIn each case, the benefit is paid out equally over 12 months.\n\n\nThe benefit is increased annually by 50% of the CPI, rounded up to the nearest tenth of a percent: e.g., a CPI of 2.9% gives an inflation adjustment of 1.5%. The benefit is capped below at 1% and above at 3%, so a CPI of 10% leads to a 3% inflation adjustment while a CPI of 0% leads to a 1% inflation adjustment.\nThe inflation adjustement is effective each September and the CPI used is the aggregate monthly CPI of the previous 12 months; so the September 2024 adjustment depends on the CPI from September 2023 to August 2024.\n\n\nThe ORP plan is more similar to a 401(k) plan offered by a private employer. The employee and the employer both make contributions to a retirement account which is then invested in the employee’s choice of mutual funds. Those investments grow “tax-free” until the employee begins to withdraw them upon retirement. If the employee does not use up the funds, they are passed down to that employee’s spouse, children, or other heirs; if the employee uses the funds too quickly and zeros out the account balance, no additional retirement funds are available. Though the employee hopefully still has Social Security retirement benefits and other savings to cover living expenses. This type of plan is called a defined-contribution plan as only the contributions to the retirement account are fixed by contract: the final balance depends on market factors outside of the employee’s control.\nAt retirement, the employee has access to those funds and can choose to withdraw them at any rate desired. A general rule of thumb is withdrawing 4% of the value per year, e.g., this Schwab discussion; you can assume a constant withdrawal rate in your analysis. Note that unwithdrawn funds continue to experience market returns.\nThe funds available in a ORP account depend strongly on the investments chosen. For this analysis, you can assume that the ORP participants invest in a Fidelity Freedom Fund with the following asset allocation:[^6]\n\nAge 25 to Age 49:\n\n54% US Equities\n36% International Equities\n10% Bonds\n\n\nAge 50 to Age 59:\n\n47% US Equities\n32% International Equities\n21% Bonds\n\n\nAge 60 to Age 74:\n\n34% US Equities\n23% International Equities\n43% Bonds\n\n\nAge 75 or older:\n\n19% US Equities\n13% International Equities\n62% Bonds\n6% Short-Term Debt\n\n\n\nUnder the ORP, both the employee and the employer make monthly contributions to the employee’s ORP account. These contributions are calculated as a percentage of the employee’s annual salary. Specifically, the employee contributes at the same rate as the TRS:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nThe employer contribution is fixed at:\n\n8% for the first seven years of employment at CUNY.\n10% for all years thereafter."
  },
  {
    "objectID": "mp04.html#data-sources",
    "href": "mp04.html#data-sources",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "DATA SOURCES",
    "text": "DATA SOURCES\nFor this project, the data are collected from two economic and financial resources: - AlphaVantage: a commercial stock market data provider - FRED: the Federal Reserve Economic Data repository maintained by the Federal Reserve Bank of St. Louis.\nLOAD NECESSARY PACKAGES\n\nCodeif(!require(\"httr\")) install.packages(\"httr\")\nif(!require(\"jsonlite\")) install.packages(\"jsonlite\")\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif(!require(\"plotly\")) install.packages(\"plotly\")\n\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(httr2)\nlibrary(jsonlite)\nlibrary(tidyverse)\n\n\nLOAD ENVIRONMENT VARIABLES\nI created an account in both AlphaVantage and FRED to get the apikey, and those api keys are stored as AVKEY2 and FRED_KEY in the .Renviron file.\n\nCode#TASK 1 and TASK 2\n#| warning: false\n#| message: false\n# Load environment variables\nreadRenviron(\"MP-DATA/.Renviron\")\n\n\nFunctions To Fetch From AlphaVantage and FRED\n\nCodeconvert_fred_datatype=function(fred_data){\n  fred_data=fred_data|&gt;\n    mutate(date=as.Date(date),value=as.numeric(value))|&gt;\n    group_by(date)|&gt;\n    summarise(value = mean(value, na.rm = TRUE), .groups = \"drop\")\n  return(fred_data)\n}\ncalculate_return_symbol &lt;- function(alpha_vantage_symbol_data) {\n  # Dynamically construct the name for the returns column\n  return_column_name &lt;- paste0(unique(alpha_vantage_symbol_data$symbol), \"_returns\")}\n\n# Define variables\n\nconvert_to_a_table&lt;-function(symbol_data)\n{ do.call(\n  rbind, \n  lapply(\n    names(symbol_data$`Monthly Adjusted Time Series`), \n    function(date) {\n      # Extract the record for the given date\n      record &lt;- symbol_data$`Monthly Adjusted Time Series`[[date]]\n      \n      # Check if all required fields are present\n      if (!is.null(record$`1. open`) && \n          !is.null(record$`2. high`) && \n          !is.null(record$`3. low`) && \n          !is.null(record$`4. close`) && \n          !is.null(record$`6. volume`) && \n          !is.null(record$`7. dividend amount`)) {\n        \n        # Create a data frame for non-missing records\n        return( data=data.frame(\n          date = as.Date(date),                      # Convert date string to Date\n          open = as.numeric(record$`1. open`),\n          high = as.numeric(record$`2. high`),\n          low = as.numeric(record$`3. low`),\n          close = as.numeric(record$`4. close`),\n          adjusted_close = as.numeric(record$`5. adjusted close`),\n          volume = as.numeric(record$`6. volume`),\n          dividend = as.numeric(record$`7. dividend amount`),\n          stringsAsFactors = FALSE\n        ))\n      } else {\n        # Skip records with missing fields\n        return(NULL)\n      }\n    }\n  )\n)\n}\n\nfetch_alpha_vantage &lt;- function(symbol, function_name = \"TIME_SERIES_MONTHLY_ADJUSTED\") {\n  av_key &lt;- Sys.getenv(\"AV_KEY\") ##apikey\n  url &lt;- paste0(\n    \"https://www.alphavantage.co/query?\",\n    \"function=\", function_name,\n    \"&symbol=\", symbol,\n    \"&apikey=\", av_key\n  )\n  \n  # Make the API request\n  response &lt;- GET(url)\n  \n  if(status_code(response)==200){\n    # Parse and print the JSON data\n    data &lt;- content(response, as = \"parsed\")\n    data_table&lt;-data|&gt;convert_to_a_table()\n    data_table=data_table|&gt;mutate(month=floor_date(as.Date(date), \"month\"))|&gt;group_by(month) |&gt;\n            summarize(adjusted_close = last(adjusted_close))\n    return(data_table)\n  }\n  else\n  {\n    stop(\"failed to fetch data\")\n  }\n}\n\n\n# Function to fetch data from FRED\nfetch_fred &lt;- function(series_id) {\n  fred_key &lt;- Sys.getenv(\"FRED_KEY\") ##apikey for fred\n  real_time_start=\"1980-01-01\"\n  real_time_end=Sys.Date()\n  frequency=\"m\"\n  url2 &lt;- paste0(\"https://api.stlouisfed.org/fred/series/observations?\",\n                 \"series_id=\",series_id,\n                 \"&realtime_start=\",real_time_start,\n                   \"&realtime_end=\",real_time_end,\n                 \"&frequency\",frequency,\n                 \"&api_key=\",fred_key,\n                 \"&file_type=json\")\n  response_t=GET(url2)\n  \n  \n  # Check response status\n  if (status_code(response_t) == 200) {\n    content&lt;-content(response_t)\n    my_list&lt;-content$observations|&gt;as.list()\n    my_table &lt;- do.call(rbind, lapply(my_list, function(x) {\n      data.frame(\n        realtime_start = x$realtime_start,\n        realtime_end   = x$realtime_end,\n        date           = x$date,\n        value          = x$value,\n        stringsAsFactors = FALSE\n      )\n    }))\n  \n   \n  } else {\n    stop(\"Failed to fetch data from FRED.\")\n  }\n}"
  },
  {
    "objectID": "mp04.html#data-acquisition",
    "href": "mp04.html#data-acquisition",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "DATA ACQUISITION",
    "text": "DATA ACQUISITION\nAs the next step, I have collected historical data covering - Rate of Inflation - Rate of Wage Growth - US Equity Market Returns - International Equity Market Returns - Bond Returns - Short Term Debt Returns\nFRED Data\n\nCode# Suppress warnings and messages for better readability of output\n#| warning: false  # Disable warnings\n#| message: false\n# Load necessary library\nlibrary(lubridate)  # For working with dates (e.g., extracting year from a date object)\n\n  # Disable messages\n\n# Fetching and processing inflation data\n\n# 1. **Annual Inflation (Percentage)** \n# Fetches inflation data (consumer prices for the United States) from the FRED API\n# Converts data types for compatibility and adds a new column for annual CPI percentages\nINFLATION_ANNUAL &lt;- fetch_fred(series_id = \"FPCPITOTLZGUSA\") |&gt; \n  convert_fred_datatype() |&gt; \n  mutate(Annual_CPI_Percentage = value)\n\n# 2. **Monthly Inflation Average (Urban Areas)**\n# Fetches the Consumer Price Index for All Urban Consumers from the FRED API\n# Renames the `value` column to `US_City_Monthly_Inflation_Average` and `date` to `month` for clarity\nINFLATION_US_URBAN_AVERAGE &lt;- fetch_fred(series_id = \"CPIAUCSL\") |&gt; \n  convert_fred_datatype() |&gt; \n  rename(US_City_Monthly_Inflation_Average = value, month = date)\n# Suppress warnings and messages for better readability of output\n#| warning: false  # Disable warnings\n#| message: false\n# 3. **Annual Inflation Average and Percentage Change**\n# Calculates the annual average of monthly inflation data\n# Groups data by year, computes the mean for each year, and calculates percentage changes compared to the previous year\nannual_inflation_average_percentage &lt;- INFLATION_US_URBAN_AVERAGE |&gt; \n  mutate(year = year(month)) |&gt;  # Extracts year from the month column\n  group_by(year) |&gt;  # Groups data by year for annual summary\n  summarize(\n    annual_inflation_average = mean(US_City_Monthly_Inflation_Average),  # Computes the mean inflation for each year\n    .groups = \"drop\"  # Ungroup data after summarizing\n  ) |&gt; \n  mutate(\n    perc_change_annual = (annual_inflation_average - lag(annual_inflation_average)) / lag(annual_inflation_average) * 100  # Computes year-over-year percentage change\n  )\n\n# 4. **Monthly Inflation Average (NYC Metro Area)**\n# Fetches inflation data for the NYC metropolitan area from the FRED API\n# Converts data types and adds a descriptive column indicating the type of data\nINFLATION_NYC &lt;- fetch_fred(series_id = \"CUURA101SA0\") |&gt; \n  convert_fred_datatype() |&gt; \n  mutate(type = \"NYC Monthly Average\")\n\n# Note:\n# - `fetch_fred(series_id = \"...\")`: Retrieves data from the FRED API using the specified series ID.\n# - `convert_fred_datatype()`: Converts data types to ensure compatibility with further operations.\n# - `mutate()`: Adds or modifies columns in a data frame.\n# - `group_by()`: Groups data for aggregation (e.g., summarizing by year).\n# - `summarize()`: Aggregates data (e.g., computes annual averages).\n# - `lag()`: Accesses previous row values for calculating percentage changes.\n\n\n\nCode# 5. *Average Hourly Earnings of All Employees, Total Private CES0500000003**\n# Fetches Average Hourly Earnings of All Employees, Total Private from the FRED API\n# Converts data types and adds a descriptive column indicating the type of data\nWAGE_GROWTH_DATA&lt;-fetch_fred(\"CES0500000003\")|&gt;convert_fred_datatype()|&gt;rename(wage_growth_rate=value,month=date)\n#6.*Employment Cost Index: Wages and salaries for State and local government workers in Education services (CIU3026100000000I)*\nWAGE_GROWTH_GOVT_EMPLOYEES&lt;-fetch_fred(\"CIU3026100000000I\")|&gt;convert_fred_datatype()|&gt;rename(wage_growth_rate=value,month=date)\n\n\nFunctions to calculate return:\n\nCodecalculate_return_symbol &lt;- function(alpha_vantage_symbol_data) {\n  # Dynamically construct the name for the returns column\n  # Dynamically construct the name for the returns column\n  return_column_name &lt;- paste0(unique(alpha_vantage_symbol_data$symbol), \"_returns(%)\")\n  ret_vect&lt;-alpha_vantage_symbol_data|&gt;mutate(value=(adjusted_close/lag(adjusted_close)-1)*100,value=round(value,2))|&gt;\n    rename(!!return_column_name := value)\n  return(ret_vect)\n}\n\n\nFunctions to calculate long run averages\n\nCodelong_run_monthly_avg &lt;- function(returns_df){\n  return_column_name &lt;- paste0(unique(returns_df$symbol), \"_returns(%)\")\n  rect_df&lt;-returns_df |&gt;\n  mutate(month_only = format(month, \"%m\")) |&gt;  # Extract month (e.g., \"01\" for January)\n  group_by(month_only) |&gt;                     # Group by month\n  summarise(\n    monthly_avg_return = mean(.data[[return_column_name]], na.rm = TRUE) # Calculate the average\n  )\n  return(rect_df)\n}\n\n\nALPHAVANTAGE Data\n\nCode# Fetching US Equity Market Data Using ETFs\n\n# 1. **S&P 500 ETF (SPY)**\n# Fetches data for SPY ETF from Alpha Vantage, representing the S&P 500 index as a proxy for the US equity market.\nSPY_ETF &lt;- fetch_alpha_vantage(symbol = \"SPY\")  # Retrieves SPY ETF data\nSPY_ETF$symbol &lt;- \"SPY\"  # Adds a new column identifying the symbol for clarity\n\n# 2. **Nasdaq-100 ETF (QQQ)**\n# Fetches data for QQQ ETF from Alpha Vantage, representing the Nasdaq-100 index as another proxy for US equity performance.\nQQQ_ETF &lt;- fetch_alpha_vantage(symbol = \"QQQ\")  # Retrieves QQQ ETF data\nQQQ_ETF$symbol &lt;- \"QQQ\"  # Adds a new column identifying the symbol for clarity\n\n# 3. **Dow Jones Industrial Average ETF (DIA)**\n# Fetches data for DIA ETF from Alpha Vantage, representing the Dow Jones Industrial Average as a proxy for US equity.\nDIA_ETF &lt;- fetch_alpha_vantage(symbol = \"DIA\")  # Retrieves DIA ETF data\nDIA_ETF$symbol &lt;- \"DIA\"  # Adds a new column identifying the symbol for clarity\n\n# Calculating Percentage Returns for Each ETF\n\n# 4. **Calculate Returns for SPY**\n# Computes percentage returns for the SPY ETF.\nusequity1.perc.returns &lt;- calculate_return_symbol(SPY_ETF)\n\n# 5. **Calculate Returns for QQQ**\n# Computes percentage returns for the QQQ ETF.\nusequity2.perc.returns &lt;- calculate_return_symbol(QQQ_ETF)\n\n# 6. **Calculate Returns for DIA**\n# Computes percentage returns for the DIA ETF.\nusequity3.perc.returns &lt;- calculate_return_symbol(DIA_ETF)\n\n# Combining and Aggregating US Equity Market Returns\n\n# 7. **Combine Returns from All ETFs**\n# Creates a list of percentage returns from SPY, QQQ, and DIA.\n# Selects only the `month` and the respective return columns for clarity.\nusequity.all.perc.returns &lt;- list(\n  usequity1.perc.returns |&gt; select(month, `SPY_returns(%)`),\n  usequity2.perc.returns |&gt; select(month, `QQQ_returns(%)`),\n  usequity3.perc.returns |&gt; select(month, `DIA_returns(%)`)\n) |&gt;\n  reduce(full_join, by = \"month\") |&gt;  # Merges all three return datasets by `month`\n  arrange(month) |&gt;                   # Ensures data is sorted by `month`\n  drop_na() |&gt;                        # Removes rows with missing data\n  mutate(\n    # Calculates the average return across SPY, QQQ, and DIA to estimate the US equity market return\n    US_equity_market_return = rowMeans(across(c(`SPY_returns(%)`, `QQQ_returns(%)`, `DIA_returns(%)`)), na.rm = TRUE)\n  )\n\n# Explanation of Functions and Symbols:\n# - `fetch_alpha_vantage(symbol)`: Fetches ETF data from Alpha Vantage using the specified `symbol`.\n# - `$symbol`: Adds a new column to label the ETF for later identification.\n# - `calculate_return_symbol()`: Computes percentage returns for the given ETF data.\n# - `list()`: Combines selected data frames into a list.\n# - `select()`: Extracts specific columns (e.g., `month` and returns) for clarity.\n# - `reduce(full_join, by = \"month\")`: Merges multiple data frames by the `month` column.\n# - `arrange(month)`: Ensures the data is chronologically ordered.\n# - `drop_na()`: Removes rows with missing values to ensure accurate calculations.\n# - `mutate()`: Creates or modifies columns, e.g., computing the average market return.\n# - `rowMeans(across(...))`: Computes the average of specified columns row-wise, excluding `NA` values.\n\n\n\nCode# Suppress Warnings and Messages During Execution\n#| warning: false  # Prevents warning messages from being displayed during code execution\n#| message: false  # Suppresses informational messages generated by the functions\n\n# Fetching International Equity Market Data Using ETFs\n\n# 1. **Vanguard Developed Markets Index Fund ETF (VTMGX)**\n# Fetches data for the VTMGX ETF, which tracks international developed market equities.\nVTMGX_INT &lt;- fetch_alpha_vantage(symbol = \"VTMGX\") |&gt; \n  mutate(symbol = \"VTMGX\")  # Adds a column for identifying the ETF symbol\n\n# 2. **Vanguard Emerging Markets Stock Index Fund ETF (VEIEX)**\n# Fetches data for the VEIEX ETF, which tracks emerging market equities.\nVEIEX_INT &lt;- fetch_alpha_vantage(symbol = \"VEIEX\") |&gt; \n  mutate(symbol = \"VEIEX\")  # Adds a column for identifying the ETF symbol\n\n# Calculating Percentage Returns for Each International ETF\n\n# 3. **Calculate Returns for VTMGX**\n# Computes percentage returns for the VTMGX ETF.\nint1.perc.returns &lt;- calculate_return_symbol(VTMGX_INT)\n\n# 4. **Calculate Returns for VEIEX**\n# Computes percentage returns for the VEIEX ETF.\nint2.perc.returns &lt;- calculate_return_symbol(VEIEX_INT)\n\n# Combining and Aggregating International Equity Market Returns\n\n# 5. **Combine Returns from VTMGX and VEIEX**\n# Creates a list containing the percentage returns for both international ETFs.\n# Selects only the `month` and respective return columns for clarity.\nint.all.perc.returns &lt;- list(\n  int1.perc.returns |&gt; select(month, `VTMGX_returns(%)`),  # Selects monthly returns for VTMGX\n  int2.perc.returns |&gt; select(month, `VEIEX_returns(%)`)   # Selects monthly returns for VEIEX\n)|&gt;reduce(full_join, by = \"month\")|&gt;  # Merges the data frames by `month` column\n  arrange(month)|&gt;                   # Ensures the data is sorted chronologically\n  drop_na()|&gt;                        # Removes rows with missing values\n  mutate(\n    # Calculates the average return across VTMGX and VEIEX as a proxy for the international equity market return\n    Int_equity_market_return = rowMeans(across(c(`VTMGX_returns(%)`, `VEIEX_returns(%)`)), na.rm = TRUE)\n  )\n\n# Explanation of Functions and Symbols:\n# - `fetch_alpha_vantage(symbol)`: Fetches ETF data from Alpha Vantage for the specified `symbol`.\n# - `mutate(symbol = ...)`: Adds a column to label the ETF for easier identification.\n# - `calculate_return_symbol()`: Computes percentage returns for the given ETF data.\n# - `list()`: Combines selected data frames into a list.\n# - `select()`: Extracts specific columns (e.g., `month` and returns) for clarity.\n# - `reduce(full_join, by = \"month\")`: Combines multiple data frames by the `month` column.\n# - `arrange(month)`: Sorts the data chronologically by `month`.\n# - `drop_na()`: Excludes rows with missing data for accurate calculations.\n# - `mutate()`: Creates or modifies columns, e.g., calculating the average international market return.\n# - `rowMeans(across(...))`: Computes the row-wise average of specified columns, ignoring `NA` values.\n\n\n\nCode# Suppress Warnings and Messages During Execution\n#| warning: false  # Disables warnings\n#| message: false  # Suppresses messages\n\n# 1. **Aggregate Bond Index ETF (AGG)**\n# Fetches data for AGG ETF, a proxy for the aggregate bond market.\nAGG_BONDS &lt;- fetch_alpha_vantage(symbol = \"AGG\") |&gt; \n  mutate(symbol = \"AGG\")  # Adds a column to identify the ETF symbol.\n\n# 2. **Vanguard Intermediate-Term Corporate Bond Index Fund (VICSX)**\n# Fetches data for VICSX ETF, representing corporate bond performance.\nVICSX_BONDS &lt;- fetch_alpha_vantage(symbol = \"VICSX\") |&gt; \n  mutate(symbol = \"VICSX\")  # Adds a column to identify the ETF symbol.\n\n# 3. **Calculating Percentage Returns**\n# Computes percentage returns for the AGG and VICSX ETFs.\nbond1.perc.returns &lt;- calculate_return_symbol(AGG_BONDS)\nbond2.perc.returns &lt;- calculate_return_symbol(VICSX_BONDS)\n\n# 4. **Combining and Aggregating Bond Returns**\n# Merges the percentage returns for AGG and VICSX, calculates average bond market returns.\nbonds.all.perc.returns &lt;- list(\n  bond1.perc.returns |&gt; select(month, `AGG_returns(%)`),   # Selects AGG monthly returns\n  bond2.perc.returns |&gt; select(month, `VICSX_returns(%)`)  # Selects VICSX monthly returns\n) |&gt;\n  reduce(full_join, by = \"month\") |&gt;  # Combines the data frames on `month`\n  arrange(month) |&gt;                   # Sorts by `month`\n  drop_na() |&gt;                        # Removes rows with missing values\n  mutate(\n    # Calculates average bond market return\n    Bonds_equity_market_return = rowMeans(across(c(`AGG_returns(%)`, `VICSX_returns(%)`)), na.rm = TRUE)\n  )\n\n\n\nCode# Suppress Warnings and Messages During Execution\n#| warning: false  # Disables warnings\n#| message: false  # Suppresses messages\n\n# 1. **Vanguard Short-Term Bond Index Fund (VSGDX)**\n# Fetches data for the VSGDX ETF, representing short-term bond performance.\nVSGDX_SHORT_TERM &lt;- fetch_alpha_vantage(symbol = \"VSGDX\") |&gt; \n  mutate(symbol = \"VSGDX\")  # Adds a column to identify the ETF symbol.\n\n# 2. **Calculate Short-Term Returns**\n# Computes percentage returns for the VSGDX ETF.\nshort.term.perc.returns &lt;- calculate_return_symbol(VSGDX_SHORT_TERM)\n\n# Optional: Calculate long-run monthly averages if needed\n# #short.term.long.run.monthly.returns\n# #long_run_monthly_avg(short.term.perc.returns)\n\n\n\nCode# Suppress Warnings and Messages During Execution\n#| warning: false  # Disables warnings\n#| message: false  # Suppresses messages\n\n# Combine all relevant data sources into a single unified dataset.\nall.data &lt;- list(\n  WAGE_GROWTH_DATA,  # Data on wage growth\n  INFLATION_US_URBAN_AVERAGE |&gt; select(month, US_City_Monthly_Inflation_Average),  # Inflation data\n  usequity.all.perc.returns |&gt; select(month, US_equity_market_return),  # US equity market returns\n  int.all.perc.returns |&gt; select(month, Int_equity_market_return),  # International equity market returns\n  bonds.all.perc.returns |&gt; select(month, Bonds_equity_market_return),  # Bond market returns\n  short.term.perc.returns |&gt; select(month, `VSGDX_returns(%)`)  # Short-term bond returns\n) |&gt;\n  reduce(full_join, by = \"month\") |&gt;  # Merges datasets on `month`\n  arrange(month) |&gt;                   # Ensures chronological order\n  drop_na()                           # Removes rows with missing values"
  },
  {
    "objectID": "mp04.html#initial-analysis-and-visualization-of-input-data",
    "href": "mp04.html#initial-analysis-and-visualization-of-input-data",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "INITIAL ANALYSIS AND VISUALIZATION OF INPUT DATA",
    "text": "INITIAL ANALYSIS AND VISUALIZATION OF INPUT DATA\nComparing the Inflation in NYC and US Cities Average (1990-2024)\n\nCodeinflation_us_urban_average=INFLATION_US_URBAN_AVERAGE|&gt;rename(value=US_City_Monthly_Inflation_Average,date=month)|&gt;mutate(type=\"US_City_Monthly_Inflation_Average\")\nINFLATION_ALL=rbind(inflation_us_urban_average,INFLATION_NYC)\nggplot (data = INFLATION_ALL|&gt;filter(year(date)&gt;=1990), aes(x=date, y=value,color=type))+\n  geom_line(linewidth=0.75)+\n  theme_light()+\n  labs (title=\"Comparing the Inflation in NYC and US Cities Average (1990-2024)\",\n        y=\"Index 1982-1984=100, Not Seasonally Adjusted\",\n        x= \"year\",\n        caption = \"FRED IDs: CPIAUCSL & CUURA101SA0\")+\n  theme(text= element_text(family=\"serif\"),\n        plot.title = element_text(size=12, face=\"bold\"),\n        axis.title = element_text(size=8, face=\"italic\"),\n        plot.caption = element_text(size=6, face=\"italic\"),\n        plot.title.position = \"plot\")\n\n\n\n\n\n\n\nMontly Adjusted Close Prices of SPY, QQQ, and DIA (US MARKET EQUITY)\n\nCodeUS_MARKET_EQUITY=rbind(SPY_ETF,QQQ_ETF,DIA_ETF)\n\ngg&lt;-ggplot(US_MARKET_EQUITY, aes(x = month, y = adjusted_close, color = symbol, group = symbol)) +\n  geom_line(size = 1.2)  +\n  labs(\n    title = \"Montly Adjusted Close Prices of SPY, QQQ, and DIA (US MARKET EQUITY)\",\n    x = \"Date\",\n    y = \"Adjusted Close Price\"\n  ) +\n  theme_bw()\nggplotly(gg)\n\n\n\n\n\nMontly Adjusted Close Prices of VTMGX,VEIEX(INTERNATIONAL_EQUITY)\n\nCodeINTERNATIONAL_MARKET_EQUITY=rbind(VTMGX_INT,VEIEX_INT)\n\ngg2&lt;-ggplot(INTERNATIONAL_MARKET_EQUITY, aes(x = month, y = adjusted_close, color = symbol, group = symbol)) +\n  geom_line(size = 1.2)  +\n  labs(\n    title = \"Montly Adjusted Close Prices of VTMGX,VEIEX(INTERNATIONAL_EQUITY)\",\n    x = \"Date\",\n    y = \"Adjusted Close Price\"\n  ) +\n  theme_bw()\nggplotly(gg2)\n\n\n\n\n\nMontly Adjusted Close Prices of AGG and VICSX(BOND MARKET EQUITY)\n\nCodeBOND_MARKET_EQUITY=rbind(AGG_BONDS,VICSX_BONDS)\n\ngg3&lt;-ggplot(BOND_MARKET_EQUITY, aes(x = month, y = adjusted_close, color = symbol, group = symbol)) +\n  geom_line(size = 1.2)  +\n  labs(\n    title = \"Montly Adjusted Close Prices of AGG and VICSX(BOND MARKET EQUITY)\",\n    x = \"Date\",\n    y = \"Adjusted Close Price\"\n  ) +\n  theme_bw()\nggplotly(gg3)\n\n\n\n\n\nLog10 Monthly Adjusted Close Prices for Bond, International, and US Market Equities\n\nCodeALL.DATA=rbind(BOND_MARKET_EQUITY|&gt;mutate(type=\"BONDS\"),INTERNATIONAL_MARKET_EQUITY|&gt;mutate(type=\"INTERNATIONAL\"),US_MARKET_EQUITY|&gt;mutate(type=\"US MARKET\"))\n\ngg4&lt;-ggplot(ALL.DATA, aes(x = month, y = log10(adjusted_close), color = type, group = symbol)) +\n  geom_line(size = 1.2)  +\n  labs(\n    title = \"Log10 Monthly Adjusted Close Prices for Bond, International, and US Market Equities\",\n    x = \"Date\",\n    y = \"Log10 Adjusted Close Price\"\n  ) +\n  theme_bw()\nggplotly(gg4)\n\n\n\n\n\n\nThe Bonds Market Equity line exhibits smoother, more stable growth compared to the others, reflecting the generally lower volatility in bond markets.\n\nUS Market Equity show greater volatility and fluctuation, characteristic of stock market performance, where prices tend to experience sharper rises and declines.\n\nInternational Market Equity provides insights into global economic trends, potentially reflecting periods of growth or decline in different countries or regions. \nThe log10 transformation allows for a more uniform view of percentage changes, making the comparison of relative growth rates more apparent than comparing raw values.\n\nEqual-Weighted U.S. Market Equity Monthly and Annual Returns (SPY, QQQ, DIA)\n\nCode#| warning: false\n#| message: false\ncalculate.return=function(alpha_vantage_symbol_data){\n  ret_vect&lt;-alpha_vantage_symbol_data$adjusted_close/lag(alpha_vantage_symbol_data$adjusted_close)-1\n  return(ret_vect)\n}\n#| warning: false\n#| message: false\ncalculate_monthly_average_return=function(alpha_vantage_symbol_data){\n  ret_df=alpha_vantage_symbol_data|&gt;group_by(month)|&gt;\n    summarize(average_monthly_return=round(mean(returns,na.rm=TRUE),3),\n              `average_monthly_return_(%)`=average_monthly_return*100)|&gt;na.omit()|&gt;  # Remove NA values for better plotting\n    mutate(direction = ifelse(`average_monthly_return_(%)` &gt;= 0, \"Increasing\", \"Decreasing\"),\n           year = year(month), \n           month = as.Date(paste0(month, \"-01\")))\n  return(ret_df)\n}\n#| warning: false\n#| message: false\ncalculate_annual_average_return=function(average_return_data){\n  rect_df&lt;-average_return_data %&gt;%\n    group_by(year) %&gt;%\n    summarise(`annual_return_(%)` = round((prod(1 + average_monthly_return, na.rm = TRUE)^(1 / 12) - 1),5)*100)\n  return(rect_df)\n}\n#| warning: false\n#| message: false\nUS_MARKET_EQUITY_RETURNS&lt;-US_MARKET_EQUITY |&gt;\n  group_by(symbol) |&gt;\n  mutate(returns = calculate.return(cur_data()))|&gt;calculate_monthly_average_return()\nUS_MARKET_EQUITY_RETURNS&lt;-US_MARKET_EQUITY |&gt;\n  group_by(symbol) |&gt;\n  mutate(returns = calculate.return(cur_data()))|&gt;calculate_monthly_average_return()\n\n# Calculate the geometric average annual return (you can choose simple)\nannual_return &lt;-US_MARKET_EQUITY_RETURNS|&gt;calculate_annual_average_return()\n\n# Merge monthly returns with the annual returns\nUS_MARKET_EQUITY_RETURNS &lt;- left_join(US_MARKET_EQUITY_RETURNS, annual_return, by=c( \"year\"=\"year\"))\n\nggg&lt;-ggplot(US_MARKET_EQUITY_RETURNS, aes(x = month, y = `average_monthly_return_(%)`)) +\n  geom_col(aes(fill = direction), size = 1.2) +  # Monthly returns as bars\n  geom_line(aes(x=month,y =`annual_return_(%)` , color = \"Annual Return %\"), size = 1.5) +  # Annual return as a line\n  scale_fill_manual(values = c(\"Increasing\" = \"green\", \"Decreasing\" = \"red\")) +\n  scale_color_manual(values = c(\"Annual Return %\" = \"blue\")) +  # Color for the annual return line\n  labs(\n    title = \"Equal-Weighted U.S. Market Equity Monthly and Annual Returns (SPY, QQQ, DIA)\",\n    x = \"Month\",\n    y = \"Return (%)\",\n    fill = \"Monthly Return Direction\",\n    ) +\n  scale_x_date(date_labels = \"%b %Y\", date_breaks = \"6 months\" )+  # Format x-axis as months\n  theme_minimal(base_size = 10) +  # Larger font size for better readability\n  theme(\n    axis.text.x = element_text(hjust = 1),  # Rotate x-axis labels for better readability\n    panel.grid.major = element_line(size = 0.5, color = \"grey\"),  # Customize gridlines\n    panel.grid.minor = element_blank()  # Remove minor gridlines for a cleaner look\n  )\n\nrange_from&lt;-as.Date(US_MARKET_EQUITY_RETURNS$year)\nggplotly(ggg, dynamicTicks = TRUE) |&gt;\n  layout(\n    xaxis = list(rangeslider = list(borderwidth = 1)),  # Add range slider to x-axis\n    hovermode = \"x\",\n    yaxis = list(value = \".2f\") ) # Format ticks as percentage\n\n\n\n\n\nCorrelation between key economic indicators\n\nCode# Calculate the correlation matrix\ncorrelation_matrix &lt;- all.data %&gt;%\n  select(-month) %&gt;%\n  cor(use = \"complete.obs\")\n\n# Prepare the data for plotting\ncorrelation_heatmap &lt;- as.data.frame(correlation_matrix) %&gt;%\n  rownames_to_column(var = \"variable1\") %&gt;%\n  pivot_longer(cols = -variable1, names_to = \"variable2\", values_to = \"correlation\")\n\n# Create the heatmap with values displayed\nggplot(correlation_heatmap, aes(x = variable1, y = variable2, fill = correlation)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = round(correlation, 3)), color = \"black\", size = 3) +  # Add correlation values\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0) +\n  labs(\n    title = \"Correlation Heatmap\",\n    subtitle = \"Correlation between key economic indicators\",\n    x = \"Variable\", \n    y = \"Variable\", \n    fill = \"Correlation\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid = element_blank()\n  )\n\n\n\n\n\n\n\n\nwage_growth_rate and US_City_Monthly_Inflation_Average have a high correlation (0.9878), suggesting that as inflation increases, wage growth tends to increase.\n\nUS_equity_market_return and wage_growth_rate (0.0184) show almost no relationship. Bonds_equity_market_return and wage_growth_rate (-0.0960) exhibit a slight inverse relationship.\nUS_equity_market_return and Int_equity_market_return have a strong positive correlation (0.8176), indicating that international and US equity markets move similarly.\n\nBonds_equity_market_return and VSGDX_returns(%) have a strong positive correlation (0.8354), reflecting alignment between bond and specific equity market returns.  -Wage Growth & Inflation: The high correlation (0.9878) between wage growth and inflation suggests that wage adjustments might follow inflation trends.\n\nEquities & Bonds: The mixed correlations (positive and negative) across equity and bond returns reflect diverse market dynamics."
  },
  {
    "objectID": "mp04.html#historical-comparison-of-trs-and-orp",
    "href": "mp04.html#historical-comparison-of-trs-and-orp",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "HISTORICAL COMPARISON OF TRS and ORP",
    "text": "HISTORICAL COMPARISON OF TRS and ORP\nI am trying to do a comparison of TRS and ORP with collected historical data. The assumption is the CUNY employee joined the first month of the historical data and retired from CUNY at the end of the final month of data.\n\nCode#|warning: false\n\n\n\n\n##function to calculate TRS\ncalculate_trs &lt;- function(starting_salary, wage_growth_data, inflation_data, years_of_service, retirement_benefit_years) {\n  \n  salary &lt;- starting_salary\n  total_contributions &lt;- 0\n  salaries &lt;- numeric(years_of_service)\n  \n  # Helper function: Calculate biweekly employee contributions\n  employee_contribution &lt;- function(salary) {\n    emp_rate &lt;- ifelse(salary &lt;= 45000, 0.03, \n                       ifelse(salary &lt;= 55000, 0.035,\n                              ifelse(salary &lt;= 75000, 0.045,\n                                     ifelse(salary &lt;= 100000, 0.0575, 0.06))))\n    return(salary * emp_rate / 26)\n  }\n  \n  # Loop to calculate salary progression and contributions\n  for (i in 1:years_of_service) {\n    annual_increase &lt;- wage_growth_data$annual_ECI_perct[i] / 100\n    cpi_rate &lt;- inflation_data$annual_CPI_perct[i] / 100\n    salary &lt;- salary * (1 + annual_increase + cpi_rate)\n    salaries[i] &lt;- salary\n    \n    biweekly_contributions &lt;- employee_contribution(salary)\n    total_contributions &lt;- total_contributions + biweekly_contributions * 26\n  }\n  \n  # Print Salary Progression for Debugging\n  print(\"\\n ::::Year Salary Progression::::::: \\n\")\n  print(salaries)\n  \n\n  \n  # Calculate Final Average Salary (FAS)\n  last_3_salaries &lt;- tail(salaries, 3)\n  FAS &lt;- ifelse(length(last_3_salaries) &lt; 3, mean(salaries), mean(last_3_salaries))\n  \n  # Helper function: Calculate retirement benefit\n  annual_retirement_benefit &lt;- function(FAS, years_of_service, retirement_benefit_years, inflation_data) {\n    if (years_of_service &lt;= 20) {\n      benefit &lt;- 0.0167 * FAS * years_of_service\n    } else if (years_of_service == 20) {\n      benefit &lt;- 0.0175 * FAS * years_of_service\n    } else {\n      benefit &lt;- (0.35 + 0.02 * (years_of_service - 20)) * FAS\n    }\n    return(benefit)\n  }\n    \n\n  \n  # Calculate the adjusted retirement benefit\n  annual_benefit &lt;- annual_retirement_benefit(FAS, years_of_service, retirement_benefit_years, inflation_data)\n  \n  # Print results\n  cat(\"\\n Years of Service:\", years_of_service, \"\\n\")\n  cat(\"Final Average Salary (FAS): $\", round(FAS, 2), \"\\n\")\n  cat(\"Total Employee Contributions: $\", round(total_contributions, 2), \"\\n\")\n  cat(\"Annual Retirement Benefit for without Adjustment\", retirement_benefit_years, \"years: $\", annual_benefit, \"\\n\")\n  cat(\"Monthly Retirement Benefit for without Adjustment\", retirement_benefit_years, \"years: $\", annual_benefit/12, \"\\n\")\n  return(annual_benefit)\n}\n\n\n\nCode# Define a function to calculate the asset allocation based on age\ncalculate_allocation &lt;- function(age) {\n  \n  # Default asset allocation for all categories\n  stocks1_optperc &lt;- 0.27 / 0.55 * 0.7\n  stocks2_optperc &lt;- 0.14 / 0.55 * 0.7\n  stocks3_optperc &lt;- 0.14 / 0.55 * 0.7\n  stocks_optperc &lt;- stocks1_optperc + stocks2_optperc + stocks3_optperc\n  \n  international1_optperc &lt;- 0.075 / 0.275 * 0.15\n  international2_optperc &lt;- 0.075 / 0.275 * 0.15\n  international3_optperc &lt;- 0.125 / 0.275 * 0.15\n  international_optperc &lt;- international1_optperc + international2_optperc + international3_optperc\n  \n  bond_optperc &lt;-0\n  shortterm_optperc &lt;-0\n  \n  # Total allocation\n  total_optperc &lt;- stocks_optperc + international_optperc + bond_optperc + shortterm_optperc\n  \n  # Determine asset allocation based on age\n  if (age &gt;= 25 & age &lt;= 49) {\n    # Age 25-49: 54% US Equities, 36% International Equities, 10% Bonds\n    stocks_optperc &lt;- 0.54\n    international_optperc &lt;- 0.36\n    bond_optperc &lt;- 0.10\n    \n      \n  } else if (age &gt;= 50 & age &lt;= 59) {\n    # Age 50-59: 47% US Equities, 32% International Equities, 21% Bonds\n    stocks_optperc &lt;- 0.47\n    international_optperc &lt;- 0.32\n    bond_optperc &lt;- 0.21\n   \n  } else if (age &gt;= 60 & age &lt;= 74) {\n    # Age 60-74: 34% US Equities, 23% International Equities, 43% Bonds\n    stocks_optperc &lt;- 0.34\n    international_optperc &lt;- 0.23\n    bond_optperc &lt;- 0.43\n    \n  } else if (age &gt;= 75) {\n    # Age 75 and older: 19% US Equities, 13% International Equities, 62% Bonds, 6% Short-Term Debt\n    stocks_optperc &lt;- 0.19\n    international_optperc &lt;- 0.13\n    bond_optperc &lt;- 0.62\n    shortterm_optperc &lt;- 0.06\n  }\n  \n  # Calculate total allocation for the specified age range\n  total_optperc &lt;- stocks_optperc + international_optperc + bond_optperc + shortterm_optperc\n  \n  # Return the calculated allocation as a list\n  allocation &lt;- list(\n    stocks = stocks_optperc,\n    international = international_optperc,\n    bonds = bond_optperc,\n    short_term = shortterm_optperc,\n    total = total_optperc\n  )\n  \n  return(allocation)\n}\n\n\n\nCode#| warning: false\n#| message: false\n\n# Asset returns (annual average return in % for stocks, international, bonds, short term)\nasset_returns &lt;- c(stocks = all.data$US_equity_market_return, international = all.data$Int_equity_market_return, bonds = all.data$Bonds_equity_market_return, short_term = all.data$`VSGDX_returns(%)`)\ncalculate_orp_with_age &lt;- function(starting_salary, starting_age, wage_growth_data, \n                                   inflation_data, years_of_service, asset_returns) {\n  \n  salary &lt;- starting_salary\n  total_balance &lt;- 0\n  current_age &lt;- starting_age\n  \n  # Check for NA values in wage_growth_data and inflation_data\n  if (any(is.na(wage_growth_data$annual_ECI_perct))) {\n    warning(\"Wage growth data contains NA values. Please check the data.\")\n  }\n  if (any(is.na(inflation_data$annual_CPI_perct))) {\n    warning(\"Inflation data contains NA values. Please check the data.\")\n  }\n  \n  # Helper function: Calculate employee contribution rate\n  employee_contribution_rate &lt;- function(salary) {\n    if (is.na(salary)) {\n      stop(\"Salary is NA. Please provide a valid salary.\")  # Stop execution if salary is NA\n    }\n    if (salary &lt;= 45000) return(0.03)\n    else if (salary &lt;= 55000) return(0.035)\n    else if (salary &lt;= 75000) return(0.045)\n    else if (salary &lt;= 100000) return(0.0575)\n    else return(0.06)\n  }\n  \n  # Loop through each year of service\n  for (i in 1:years_of_service) {\n    # Ensure wage_growth_data and inflation_data have valid entries\n    if (i &lt;= nrow(wage_growth_data)) {\n      annual_increase &lt;- wage_growth_data$annual_ECI_perct[i] / 100\n    } else {\n      annual_increase &lt;- 0\n    }\n    \n    if (i &lt;= nrow(inflation_data)) {\n      cpi_rate &lt;- inflation_data$annual_CPI_perct[i] / 100\n    } else {\n      cpi_rate &lt;- 0\n    }\n    \n    # Update salary considering wage growth and inflation\n    salary &lt;- salary * (1 + annual_increase + cpi_rate)\n    \n    # Validate salary before calculating contributions\n    if (is.na(salary) || salary &lt;= 0) {\n      stop(\"Salary is invalid or less than zero after adjustments. Please check the data.\")\n    }\n    \n    emp_rate &lt;- employee_contribution_rate(salary)\n    emp_contrib &lt;- emp_rate * salary\n    emp_monthly &lt;- emp_contrib / 12\n    \n    employer_rate &lt;- ifelse(i &lt;= 7, 0.08, 0.10)\n    employer_contrib &lt;- employer_rate * salary\n    employer_monthly &lt;- employer_contrib / 12\n    \n    monthly_contrib &lt;- emp_monthly + employer_monthly\n    \n    # Get the asset allocation based on the current age\n    allocation &lt;- calculate_allocation(current_age)\n    \n    # Annual growth of the portfolio based on asset returns and allocation\n    annual_growth_rate &lt;- sum(c(allocation$stocks/100, allocation$international/100, \n                                allocation$bonds/100, allocation$short_term/100) * asset_returns) / 100\n    total_balance &lt;- (total_balance + monthly_contrib * 12) * (1 + annual_growth_rate)\n    \n    current_age &lt;- current_age + 1  # Increment age by 1 year\n  }\n  \n  # Calculate 4% withdrawal strategy\n  annual_payout &lt;- total_balance * 0.04\n  monthly_payout &lt;- annual_payout / 12\n  \n  # Return total balance at retirement and the payouts\n  result &lt;- list(\n    total_balance = total_balance,\n    annual_payout = annual_payout,\n    monthly_payout = monthly_payout\n  )\n  \n  return(result)\n}\n\n\nTesting the functions for the hypothetical employee\n\nCode# Example usage:\nstarting_salary &lt;- 50000\n\n# Compute the annual ECI and CPI\nwage_growth_data &lt;- WAGE_GROWTH_GOVT_EMPLOYEES |&gt;\n  select(month, wage_growth_rate) |&gt;\n  mutate(year = year(month)) |&gt;\n  group_by(year) |&gt;\n  summarize(annual_ECI = mean(wage_growth_rate, na.rm = TRUE)) |&gt;\n  mutate(annual_ECI_perct = ((annual_ECI - lag(annual_ECI)) / lag(annual_ECI)) * 100) |&gt;\n  filter(year &gt;= year(min(all.data$month))) |&gt;\n  na.omit()\n\ninflation_data &lt;- INFLATION_US_URBAN_AVERAGE |&gt;\n  select(month, US_City_Monthly_Inflation_Average) |&gt;\n  mutate(year = year(month)) |&gt;\n  group_by(year) |&gt;\n  summarize(annual_CPI = mean(US_City_Monthly_Inflation_Average, na.rm = TRUE)) |&gt;\n  mutate(annual_CPI_perct = ((annual_CPI - lag(annual_CPI)) / lag(annual_CPI)) * 100) |&gt;\n  filter(year &gt;= year(min(all.data$month))) |&gt;\n  na.omit()\n\n# Calculate years of service\nyears_of_service &lt;- as.integer(difftime(max(all.data$month), min(all.data$month), units = \"days\") / 365.25)\nretirement_benefit_years &lt;- 15 #when I joined the data, it seems like the starting year is 2010. hence 15 years expecting the employee joined around 2010 and retiring 2025\n# Test the function\nannual_benefit=calculate_trs(starting_salary, wage_growth_data, inflation_data, years_of_service, retirement_benefit_years)\n\n[1] \"\\n ::::Year Salary Progression::::::: \\n\"\n [1] 51472.86 53592.45 55234.68 56583.94 58251.44 59399.69 61231.94 63747.39\n [9] 66594.78 69547.60 71899.83 76593.04 85258.51 92544.89\n\n Years of Service: 14 \nFinal Average Salary (FAS): $ 84798.82 \nTotal Employee Contributions: $ 43617.19 \nAnnual Retirement Benefit for without Adjustment 15 years: $ 19825.96 \nMonthly Retirement Benefit for without Adjustment 15 years: $ 1652.164 \n\n\n\nCode# Adjust benefit for inflation during retirement\n    for (year in 1:retirement_benefit_years) {\n      if (year &gt; nrow(inflation_data)) break\n      cpi_adjustment &lt;- inflation_data$annual_CPI_perct[year] / 100\n      annual_benefit &lt;- annual_benefit * (1 + cpi_adjustment)\n      cat(\"Inflation Adjusted Annual Benefits for Year\",year,\":\",\"$\",annual_benefit,\"\\t Monthly Benefits:\",\"$\",annual_benefit/12,\"\\n\")\n    }\n\nInflation Adjusted Annual Benefits for Year 1 : $ 20151.4    Monthly Benefits: $ 1679.283 \nInflation Adjusted Annual Benefits for Year 2 : $ 20784.39   Monthly Benefits: $ 1732.033 \nInflation Adjusted Annual Benefits for Year 3 : $ 21215.68   Monthly Benefits: $ 1767.974 \nInflation Adjusted Annual Benefits for Year 4 : $ 21526.12   Monthly Benefits: $ 1793.843 \nInflation Adjusted Annual Benefits for Year 5 : $ 21872.79   Monthly Benefits: $ 1822.732 \nInflation Adjusted Annual Benefits for Year 6 : $ 21899.15   Monthly Benefits: $ 1824.929 \nInflation Adjusted Annual Benefits for Year 7 : $ 22176.74   Monthly Benefits: $ 1848.061 \nInflation Adjusted Annual Benefits for Year 8 : $ 22651.06   Monthly Benefits: $ 1887.588 \nInflation Adjusted Annual Benefits for Year 9 : $ 23202.59   Monthly Benefits: $ 1933.549 \nInflation Adjusted Annual Benefits for Year 10 : $ 23622.97      Monthly Benefits: $ 1968.581 \nInflation Adjusted Annual Benefits for Year 11 : $ 23918.17      Monthly Benefits: $ 1993.18 \nInflation Adjusted Annual Benefits for Year 12 : $ 25038.48      Monthly Benefits: $ 2086.54 \nInflation Adjusted Annual Benefits for Year 13 : $ 27038.79      Monthly Benefits: $ 2253.232 \nInflation Adjusted Annual Benefits for Year 14 : $ 28155.7   Monthly Benefits: $ 2346.308 \nInflation Adjusted Annual Benefits for Year 15 : $ 28953.08      Monthly Benefits: $ 2412.756 \n\n\n\nCode #Test the function with valid data\nstarting_salary &lt;- 50000\nstarting_age &lt;- 50\nyears_of_service &lt;- 15\n#annual.return=all.data|&gt;select(US_equity_market_return,month)|&gt;group_by(year(month))|&gt;\n\n\nresult &lt;- calculate_orp_with_age(starting_salary, starting_age, wage_growth_data, \n                                 inflation_data, years_of_service, asset_returns)\n\n# Print results\ncat(\"Total fund at retirement: $\", round(result$total_balance, 2), \"\\n\")\n\nTotal fund at retirement: $ 155610.9 \n\nCodecat(\"Annual payout (4% withdrawal): $\", round(result$annual_payout, 2), \"\\n\")\n\nAnnual payout (4% withdrawal): $ 6224.44 \n\nCodecat(\"Monthly payout: $\", round(result$monthly_payout, 2), \"\\n\")\n\nMonthly payout: $ 518.7 \n\n\n\nStability and Predictability: TRS is a better option for individuals seeking guaranteed income with less risk and no need for active fund management.\nHigher Potential Income: ORP is advantageous for individuals willing to manage their investments, providing higher payouts and financial flexibility, albeit with market-dependent risks."
  },
  {
    "objectID": "mp04.html#monte-carlo-simulation",
    "href": "mp04.html#monte-carlo-simulation",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "MONTE CARLO SIMULATION\n",
    "text": "MONTE CARLO SIMULATION\n\nInspired from Youtube Video I tried doing Monte Carlo Simulation for the retirement portfolio returns for 1000 days\n\nCode #|warning: false\n #|message: false     \nusequity.ret&lt;-usequity.all.perc.returns|&gt;select(US_equity_market_return)\nint.ret&lt;-int.all.perc.returns|&gt;select(Int_equity_market_return)\nbonds.ret&lt;-bonds.all.perc.returns|&gt;select(Bonds_equity_market_return)\nshort.ret&lt;-short.term.perc.returns|&gt;select(`VSGDX_returns(%)`)\nweighted.ret&lt;-(((usequity.ret$US_equity_market_return/100)*.34) + ((int.ret$Int_equity_market_return/100)*0.23)+((bonds.ret$Bonds_equity_market_return/100)*0.43)) \n      \nmonte_carlo_simulations&lt;-function(return_vect,ndays = 1000, n_sim = 100){\n  set.seed(0)\n  return_vect=1+return_vect\n  paths&lt;-replicate(n_sim,\n                   expr=round(sample(return_vect,ndays,replace=TRUE),2)\n                   )\n  #to seee if there are any tail events\n  paths&lt;-apply(paths,2,cumprod)\n  \n  paths&lt;-data.table::data.table(paths)\n  paths$days&lt;-1:nrow(paths)\n  paths&lt;-data.table::melt(paths,id.vars=\"days\")\n \n  return(paths)\n}\n\nvisualize_simulations = function(path) {\n  ggplot(path, aes(x = days, y = (value - 1) * 100, col = variable)) +\n    geom_line() +\n    theme_bw() +\n    theme(legend.position = \"none\") +\n    labs(\n      title = \"Portfolio Simulation Over Time \\n 1000 days of Post Retirement\\n Allocation(34% US Stock, 23% International Stocks, 43% Bonds)\",\n      x = \"Days Invested\",\n      y = \"Portfolio Return (%)\"\n    )\n}\n\npath=monte_carlo_simulations(\n      return_vect=weighted.ret\n)\n      visualize_simulations(path)"
  },
  {
    "objectID": "mp04.html#extra-credit",
    "href": "mp04.html#extra-credit",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "EXTRA CREDIT",
    "text": "EXTRA CREDIT\nI tried to create a shiny app for CUNY TRS Retirement Benefit Calculation. Unfortunately, I couldn’t publish it, below are the screenshot and codes for the CUNY TRS Retirement Benefit Calculator. Feel Free to replicate\n\n\n\n\n\nCode#|warning: false\n#|message: false\n#| echo: false\n# Load the shiny library\nlibrary(shiny)\n\n# Define the UI\nui &lt;- fluidPage(\n  titlePanel(\"CUNY TRS Retirement Benefit Calculator\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      numericInput(\"salary\", \"Current Annual Salary ($):\", value = 50000, min = 0),\n      numericInput(\"age\", \"Current Age:\", value = 30, min = 18, max = 70),\n      numericInput(\"retirement_age\", \"Retirement Age:\", value = 65, min = 50, max = 80),\n      numericInput(\"annual_increase\", \"Annual Salary Increase (%):\", value = 3, min = 0, max = 20),\n      numericInput(\"average_life_expectancy\", \"Average Life Expectancy (years):\", value = 85, min = 70, max = 100),\n      numericInput(\"cpi\", \"CPI (%) for Adjustment:\", value = 3, min = 0, max = 10),\n      actionButton(\"calculate\", \"Calculate\"),\n      actionButton(\"reset\",\"Reset\")\n    ),\n    \n    mainPanel(\n      h3(\"Results\"),\n      verbatimTextOutput(\"results\"),\n      plotlyOutput(\"salary_plot\") \n    )\n  )\n)\n\n# Define the server logic\nserver &lt;- function(input, output) {\n  output$results &lt;- renderPrint({\n    input$calculate # Trigger calculation when button is clicked\n    \n    salary &lt;- input$salary\n    age &lt;- input$age\n    retirement_age &lt;- input$retirement_age\n    annual_increase &lt;- input$annual_increase\n    average_life_expectancy &lt;- input$average_life_expectancy\n    cpi &lt;- input$cpi / 100 # Convert to decimal\n    \n    total_contributions &lt;- 0\n    years_of_service &lt;- retirement_age - age\n    last_3_salaries &lt;- numeric(3)\n    retirement_benefit_years &lt;- average_life_expectancy - retirement_age\n    \n    \n    # Calculate salary progression over years of service based on the annual wage growth\n    salary_progression &lt;- data.frame(year = (age + 1):(retirement_age), salary = numeric(years_of_service))\n    salary_progression$salary[1] &lt;- salary\n    # Define the employee contribution function\n    employee_contribution &lt;- function(salary) {\n      emp_rate &lt;- ifelse(salary &lt;= 45000, 0.03, \n                         ifelse(salary &lt;= 55000, 0.035,\n                                ifelse(salary &lt;= 75000, 0.045,\n                                       ifelse(salary &lt;= 100000, 0.0575, 0.06))))\n      biweekly_contribution &lt;- salary * emp_rate / 26\n      return(biweekly_contribution)\n    }\n    \n    # Loop to calculate total contributions and final average salary (FAS)\n    for (i in 1:years_of_service) {\n      salary &lt;- salary + salary * annual_increase / 100\n      biweekly_contributions &lt;- employee_contribution(salary)\n      \n      # Update the last 3 years of salary\n      if (i &gt; years_of_service - 3) {\n        last_3_salaries[i - (years_of_service - 3)] &lt;- salary\n      }\n      \n      total_contributions &lt;- total_contributions + biweekly_contributions*26\n    }\n    \n    # Compute the final average salary (FAS)\n    FAS &lt;- mean(last_3_salaries)\n    \n    # Define the annual retirement benefit function\n    annual_retirement_benefit &lt;- function(FAS, years_of_service, retirement_benefit_years) {\n      if (years_of_service &lt;= 20) {\n        benefit &lt;- 0.0167 * FAS * years_of_service\n      } else if (years_of_service == 20) {\n        benefit &lt;- 0.0175 * FAS * years_of_service\n      } else {\n        benefit &lt;- (0.35 + 0.02 * (years_of_service - 20)) * FAS\n      }\n      \n      # Apply CPI adjustment\n      cpi_adjustment &lt;- max(min(0.01 + 0.005 * cpi, 0.03), 0.01)\n      adjusted_benefit &lt;- benefit * (1 + cpi_adjustment)^retirement_benefit_years\n      \n      return(adjusted_benefit)\n    }\n    \n    # Calculate annual retirement benefit\n    adjusted_benefit &lt;- annual_retirement_benefit(FAS, years_of_service, retirement_benefit_years)\n    \n    # Print the results\n    cat(\"Years Of Services:\",years_of_service,\"/n\")\n    cat(\"Final Average Salary (FAS): $\", round(FAS, 2), \"\\n\")\n    cat(\"Total Employee Contributions: $\", round(total_contributions, 2), \"\\n\")\n    cat(\"Annual Inflation-Adjusted Retirement Benefit for\",retirement_benefit_years, \"years: $\", round(adjusted_benefit, 2), \"\\n\")\n  })\n\noutput$salary_plot &lt;- renderPlotly({\n  salary &lt;- input$salary\n  annual_increase &lt;- input$annual_increase\n  years_of_service &lt;- input$retirement_age - input$age\n  \n  # Generate yearly salary data\n  salary_data &lt;- data.frame(\n    Year = seq(1, years_of_service),\n    Salary = cumprod(rep(1 + annual_increase / 100, years_of_service)) * salary\n  )\n  \n  # Plot the salary progression\n salary_increase_graph&lt;- ggplot(salary_data, aes(x = Year, y = Salary)) +\n    geom_line(color = \"blue\", size = 1) +\n    labs(title = \"Salary Progression Over Service Years\",\n         x = \"Years of Service\",\n         y = \"Salary ($)\") +\n    theme_minimal()\n ggplotly(salary_increase_graph)\n})\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\nShiny applications not supported in static R Markdown documents"
  }
]